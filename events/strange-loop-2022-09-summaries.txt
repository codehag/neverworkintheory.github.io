Joanna

AI-based code generation tools (such as GitHub Copilot) can help software engineers save time while developing their code. However, are the code snippets generated by these tools trustworthy? In this talk, I explain some of the security and quality risks of using the code generated by these tools based on our research findings. Study pre-print: https://s2e-lab.github.io/preprints/scam22-preprint.pdf.

Tianyi

Recurrent Neural Networks (RNNs) have been widely used in NLP tasks. Yet it is challenging to debug RNNs due to their inherent complexity and opaqueness. To address this challenge, we present an interactive debugger that transforms an RNN model, which is complex and unfamiliar to regular developers, back to something they are familiar with----a Finite State Machine (FSM). The FSM provides a bird’s-eye view of the internal decision-making process of the RNN model. As the model reads each word in an input sentence, it will transit between different states until it reaches the end of the sentence. If a developer clicks on a state, they can see the frequent words and phrases associated with this state. In this way, we convert those high-dimensional arrays to symbolic values that are more interpretable to programmers. Given a misclassified text, our debugger will produce a state trace with intermediate decisions made by the RNN model. Similar to how we can step through a program, we can step through the states in the trace to inspect the decision-making process of the model.

Foutse

NeuraLint is a toolset for verifying Deep Learning (DL) models using meta-modeling and graph transformations (using Groove toolset). A DL program as input must be written using TensorFlow or Keras. First, the program is parsed to extract relevant information according to the meta-model. The model of the program is a graph that conforms to the type graph (meta-model). Then, the graph is verified by Groove as a model checker. The output graph of Groove is used to extract relevant information for the final report. Here are links to the paper and NeuraLint’s repository: <a href="https://dl.acm.org/doi/abs/10.1145/3470006">The paper</a> <a href="https://github.com/neuralint/neuralint">The GitHub repository</a>

The DeepChecker is a dynamic DL program debugger using monitored training and property-based verifications. For now, we support DL programs written using TensorFlow. The debugging is conducted through three stages. During the pre-training stage, the DeepChecker runs all prechecks on the input data and neural network’s starting state (random weights, initial loss) to validate their status or report any poor quality data processing or misconfigurations. Next, the on-training debugging stage consists of periodically running verification routines on the DL program states and metrics with the objective of detecting any property violation (wrong outputs, uncorrelated metrics) or inefficient training (unstable activation patterns, vanishing gradients). Upon successful neural network fitting verification, a post-training debugging stage performs separate inspections of involved modules like data loader and data augmentation. In order to switch on/off these checks and adjust their sensitivities, user-defined settings (YAML file format) must be updated to specify the active checks and set their associated thresholds. Throughout the debugging stages, the DeepChecker displays human-readable messages that explain violated properties and faulty states to assist the user in determining the possible root cause. Here are links to the paper and the code repository:  <a href="https://doi.org/10.1145/3529318">The paper</a> <a href="https://github.com/thedeepchecker/thedeepchecker">The GitHub repository</a>

Sarah

While software libraries avoid re-inventing the wheel, using their Application Programming Interfaces (APIs) is not always straight forward. This is especially true when there are implicit undocumented expectations on how to use the API. Building on years of research in the area of API misuse detection, this talks presents a way to create an automated continuous conversation between API designers and API users.  By mining lots of examples of how an API is used in practice, API designers get a starting point for authoring usage rules for how they expect developers to use their APIs. These rules are then automatically encoded into checks that client developers can use to ensure they are correctly using the library. Check <a href="https://sarahnadi.org/smr/api-misuse/">https://sarahnadi.org/smr/api-misuse/ </a> for more details and related tools.

Chris

<a href="https://chbrown13.github.io">Dr. Chris Brown</a> is an Assistant Professor in the <a href="https://cs.vt.edu/">Department of Computer Science</a> at Virginia Tech. His <a href="https://code-world-no-blanket.github.io/">research group</a> aims to improve the behavior, productivity, and decision-making of software engineers. This talk focuses specifically on improving the adoption of useful tools to automated development tasks. Dr. Brown provides an overview of attempted methods to recommend tools for finding and fixing software errors, proposes a framework to design effective recommendations based on <i>nudge theory</i>, and provides takeaways for researchers, tool builders, and developers to overcome "the three unwise monkeys" of tool adoption. The papers and books referenced include:

- Brown and Parnin, <a href="https://dl.acm.org/doi/10.1109/BotSE.2019.00021">Sorry to Bother You: Designing Bots for Effective Recommendations</a>.
- Brown, et al., <a href="https://ieeexplore.ieee.org/document/8103460">How Software Users Recommend Tools to Each Other</a>.
- Murphy-Hill and Murphy, <a href="https://dl.acm.org/doi/10.1145/1958824.1958888">Peer Interaction Effectively, Yet Infrequently, Enables Programmers to Discover New Tools</a>.
- Thaler and Sunstein, <a href="https://www.google.com/books/edition/Nudge/NGA9DwAAQBAJ?hl=en&gbpv=0">Nudge: Improving Decisions about Health, Wealth, and Happiness</a>.
- Maier, et al., <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2200300119">No evidence for nudging after adjusting for publication bias</a>.
- Brown and Parnin, <a href="https://dl.acm.org/doi/10.1145/3387940.3391506">Sorry to Bother You Again: Developer Recommendation Choice Architectures for Designing Effective Bots</a>.

For more information, please feel free to <a href="https://chbrown13.github.io/contact.html">reach out to Chris</a> and visit <a href="https://se-participants.github.io">SE Participants</a> to discover more ways to get involved with his research.
