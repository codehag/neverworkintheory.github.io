---
layout: post
author: Greg Wilson
title: "A Dozen in One"
date: 2023-04-26
categories: ["Editorial"]
---

<p>
  I've fallen behind on reviewing while prepping for this week's talks,
  so here are a dozen papers you might enjoy.
</p>

<p>
  Brittany Johnson, Christian Bird, Denae Ford, Nicole Forsgren, and Tom Zimmermann.
  Make your tools sparkle with trust: the PICSE framework for trust in software tools.
  In <em>ICSE SEIP</em>. May 2023, <a href="https://www.microsoft.com/en-us/research/publication/the-picse-framework-for-trust-in-software-tools/">https://www.microsoft.com/en-us/research/publication/the-picse-framework-for-trust-in-software-tools/</a>.
</p>

<blockquote>
  <p>
    The day to day of a software engineer involves a variety of tasks. While many of these tasks are collaborative and completed as such, it is not always possible or feasible to engage with other engineers for task completion. Software tools, such as code generators and static analysis tools, aim to fill this gap by providing additional support for developers to effectively complete their tasks. With a steady stream of new tools that emerging to support software engineers, including a new breed of tools that rely on artificial intelligence, there are important questions we should aim to answer regarding the trust engineers can, and should, put into their software tools and what it means to build a trustworthy tool. In this paper, we present findings from an industry interview study conducted with 18 engineers across and external to the Microsoft organization. Based on these interviews, we introduce the PICSE (pronounced "pixie") framework for trust in software tools to provide preliminary insights into factors that influence engineer trust in their software tools. We also discuss how the PICSE framework can be considered and applied in practice for designing and developing trustworthy software tools.
  </p>
</blockquote>

<p>
  Arut Prakash Kaleeswaran, Arne Nordmann, Thomas Vogel, and Lars Grunske.
  A user study for evaluation of formal verification results and their explanation at bosch.
  2023.
  <a href="https://arxiv.org/abs/2304.08950">arXiv:2304.08950</a>.
</p>

<blockquote>
  <p>
    Context: Ensuring safety for any sophisticated system is getting more complex due to the rising number of features and functionalities. This calls for formal methods to entrust confidence in such systems. Nevertheless, using formal methods in industry is demanding because of their lack of usability and the difficulty of understanding verification results. Objective: We evaluate the acceptance of formal methods by Bosch automotive engineers, particularly whether the difficulty of understanding verification results can be reduced. Method: We perform two different exploratory studies. First, we conduct a user survey to explore challenges in identifying inconsistent specifications and using formal methods by Bosch automotive engineers. Second, we perform a one-group pretest-posttest experiment to collect impressions from Bosch engineers familiar with formal methods to evaluate whether understanding verification results is simplified by our counterexample explanation approach. Results: The results from the user survey indicate that identifying refinement inconsistencies, understanding formal notations, and interpreting verification results are challenging. Nevertheless, engineers are still interested in using formal methods in real-world development processes because it could reduce the manual effort for verification. Additionally, they also believe formal methods could make the system safer. Furthermore, the one-group pretest-posttest experiment results indicate that engineers are more comfortable understanding the counterexample explanation than the raw model checker output. Limitations: The main limitation of this study is the generalizability beyond the target group of Bosch automotive engineers.
  </p>
</blockquote>

<p>
  Sarah Meldrum, Sherlock A. Licorish, Caitlin A. Owen, and Bastin Tony Roy Savarimuthu.
  Understanding stack overflow code quality: a recommendation of caution.
  <em>Science of Computer Programming</em>, 199:102516, Nov 2020, <a href="https://doi.org/10.1016/j.scico.2020.102516">https://doi.org/10.1016/j.scico.2020.102516</a>.
</p>

<blockquote>
  <p>
    Community Question and Answer (CQA) platforms use the power of online groups to solve problems, or gain information. While these websites host useful information, it is critical that the details provided on these platforms are of high quality, and that users can trust the information. This is particularly necessary for software development, given the ubiquitous use of software across all sections of contemporary society. Stack Overflow is the leading CQA platform for programmers, with a community comprising over 10 million contributors. While research confirms the popularity of Stack Overflow, concerns have been raised about the quality of answers that are provided to questions on Stack Overflow. Code snippets often contained in these answers have been investigated; however, the quality of these artefacts remains unclear. This could be problematic for the software engineering community, as evidence has shown that Stack Overflow snippets are frequently used in both open source and commercial software. This research fills this gap by evaluating the quality of code snippets on Stack Overflow. We explored various aspects of code snippet quality, including reliability and conformance to programming rules, readability, performance and security. Outcomes show variation in the quality of Stack Overflow code snippets for the different dimensions; however, overall, quality issues in Stack Overflow snippets were not always severe. Vigilance is encouraged for those reusing Stack Overflow code snippets.
  </p>
</blockquote>

<p>
  Zahra Mirzamomen and Marcel Böhme.
  Finding bug-inducing program environments.
  2023.
  <a href="https://arxiv.org/abs/2304.10044">arXiv:2304.10044</a>.
</p>

<blockquote>
  <p>
    Some bugs cannot be exposed by program inputs, but only by certain program environments. During execution, most programs access various resources, like databases, files, or devices, that are external to the program and thus part of the program’s environment. In this paper, we present a coverage-guided, mutation-based environment synthesis approach of bug-inducing program environments. Specif- ically, we observe that programs interact with their environment via dedicated system calls and propose to intercept these system calls (i) to capture the resources accessed during the first execution of an input as initial program environment, and (ii) mutate copies of these resources during subsequent executions of that input to generate slightly changed program environments. Any generated environment that is observed to increase coverage is added to the corpus of environment seeds and becomes subject to further fuzzing. Bug-inducing program environments are reported to the user.
  </p>
  <p>
    Experiments demonstrate the effectiveness of our approach. We implemented a prototype called AFLChaos which found bugs in the resource-handling code of five (5) of the seven (7) open source projects in our benchmark set (incl. OpenSSL). Automatically, AFLChaos generated environments consisting of bug-inducing databases used for storing information, bug-inducing multimedia files used for streaming, bug-inducing cryptographic keys used for encryption, and bug-inducing configuration files used to configure the program. To support open science, we publish the experimental infrastructure, our tool, and all data.},
  </p>
</blockquote>

<p>
  Lauren Olson, Emitzá Guzmán, and Florian Kunneman.
  Along the margins: marginalized communities' ethical concerns about social platforms.
  2023.
  <a href="https://arxiv.org/abs/arXiv:2304.08882">arXiv:arXiv:2304.08882</a>.
</p>

<blockquote>
  <p>
    In this paper, we identified marginalized communities' ethical concerns about social platforms. We performed this identification because recent platform malfeasance indicates that software teams prioritize shareholder concerns over user concerns. Additionally, these platform shortcomings often have devastating effects on marginalized populations. We first scraped 586 marginalized communities' subreddits, aggregated a dataset of their social platform mentions and manually annotated mentions of ethical concerns in these data. We subsequently analyzed trends in the manually annotated data and tested the extent to which ethical concerns can be automatically classified by means of natural language processing (NLP). We found that marginalized communities' ethical concerns predominantly revolve around discrimination and misrepresentation, and reveal deficiencies in current software development practices. As such, researchers and developers could use our work to further investigate these concerns and rectify current software flaws.
  </p>
</blockquote>

<p>
  Jonathan Robert Pool.
  Accessibility metatesting: comparing nine testing tools.
  2023.
  <a href="https://arxiv.org/abs/2304.07591">arXiv:2304.07591</a>, <a href="https://doi.org/10.1145/3587281.3587282">doi:10.1145/3587281.3587282</a>.
</p>

<blockquote>
  <p>
    Automated web accessibility testing tools have been found com- plementary. The implication: To catch as many issues as possible, use multiple tools. Doing this efficiently entails integration costs. Is there a small set of tools that, together, make additional tools redundant? I approach this problem by comparing nine compre- hensive accessibility testing tools that are amenable to integration: alfa, axe-core, Continuum, Equal Access, HTML CodeSniffer, Nu Html Checker, QualWeb, Tenon, and WAVE. I tested 121 web pages of interest to CVS Health with these tools. Each tool only fraction- ally duplicated any other tool. Each discovered numerous issue instances missed by all the others. Thus, testing with all nine tools was substantially more informative than testing with any subset.
  </p>
</blockquote>

<p>
  Tobias Runge, Tabea Bordis, Alex Potanin, Thomas Thüm, and Ina Schaefer.
  Flexible correct-by-construction programming.
  2022.
  <a href="https://arxiv.org/abs/arXiv:2211.15261">arXiv:arXiv:2211.15261</a>.
</p>

<blockquote>
  <p>
    Correctness-by-Construction (CbC) is an incremental program construction process to construct functionally correct programs. The programs are constructed stepwise along with a specification that is inherently guaranteed to be satisfied. CbC is complex to use without specialized tool support, since it needs a set of predefined refinement rules of fixed granularity which are additional rules on top of the programming language. Each refinement rule introduces a specific programming statement and developers cannot depart from these rules to construct programs. CbC allows to develop software in a structured and incremental way to ensure correctness, but the limited flexibility is a disadvantage of CbC. In this work, we compare classic CbC with CbC-Block and TraitCbC. Both approaches CbC-Block and TraitCbC, are related to CbC, but they have new language constructs that enable a more flexible software construction approach. We provide for both approaches a programming guideline, which similar to CbC, leads to well-structured programs. CbC-Block extends CbC by adding a refinement rule to insert any block of statements. Therefore, we introduce CbC-Block as an extension of CbC. TraitCbC implements correctness-by-construction on the basis of traits with specified methods. We formally introduce TraitCbC and prove soundness of the construction strategy. All three development approaches are qualitatively compared regarding their programming constructs, tool support, and usability to assess which is best suited for certain tasks and developers.
  </p>
</blockquote>

<p>
  Shashank Srikant, Anna A. Ivanova, Yotaro Sueoka, Hope H. Kean, Riva Dhamala, Evelina Fedorenko, Marina U. Bers, and Una-May O'Reilly.
  Program comprehension does not primarily rely on the language centers of the human brain.
  2023.
  <a href="https://arxiv.org/abs/2304.12373">arXiv:2304.12373</a>.
</p>

<blockquote>
  <p>
    Our goal is to identify brain regions involved in comprehending computer programs. We use functional magnetic resonance imaging (fMRI) to investigate two candidate systems of brain regions which may support this – the Multiple Demand (MD) system, known to respond to a range of cognitively demanding tasks, and the Language system, known to primarily respond to language stimuli. We devise experiment conditions to isolate the act of code comprehension, and employ a state-of-the-art method to locate brain systems of interest. We administer these experiments in Python (24 participants) and ScratchJr (19 participants) - which provides a visual interface to programming, thus eliminating the effect of text in code comprehension. From this robust experiment setup, we find that the Language system is not consistently involved in code comprehension, while the MD system is. Further, we find no other brain regions beyond those in the MD system to be responsive to code. We also find that variable names, the control flow used in the program, and the types of operations performed do not affect brain responses. We discuss the implications of our findings on the software engineering and CS education communities.
  </p>
</blockquote>

<p>
  Mark Swillus and Andy Zaidman.
  Deconstructing sentimental stack overflow posts through interviews: exploring the case of software testing.
  CHASE 2023 Registered Reports, 2023.
  <a href="https://arxiv.org/abs/2304.11280">arXiv:2304.11280</a>.
</p>

<blockquote>
  <p>
    The analysis of sentimental posts about software testing on Stack Overflow reveals that motivation and commitment of developers to use software testing methods is not only influenced by tools and technology. Rather, attitudes are also influenced by socio-technical factors. No prior studies have attempted to talk with Stack Overflow users about the sentimental posts that they write, yet, this is crucial to understand their experiences of which their post is only one fragment. As such, this study explores the precursors that make developers write sentimental posts about software testing on Stack Overflow. Through semi-structured interviews, we reconstruct the individual experiences of Stack Overflow users leading to sentimental posts about testing. We use the post as an anchor point to explore the events that lead to it and how users moved on in the meantime. Using strategies from socio-technical grounded theory (STGT), we derive hypotheses about the socio-technical factors that cause sentiment towards software testing.
  </p>
</blockquote>

<p>
  Xin Tan, Yiran Chen, Haohua Wu, Minghui Zhou, and Li Zhang.
  Is it enough to recommend tasks to newcomers? understanding mentoring on good first issues.
  ICSE 2023, 2023.
  <a href="https://arxiv.org/abs/2302.05058">arXiv:2302.05058</a>.
</p>

<blockquote>
  <p>
    Newcomers are critical for the success and continuity of open source software (OSS) projects. To attract newcomers and facilitate their onboarding, many OSS projects recommend tasks for newcomers, such as good first issues (GFIs). Previous studies have preliminarily investigated the effects of GFIs and techniques to identify suitable GFIs. However, it is still unclear whether just recommending tasks is enough and how significant mentoring is for newcomers. To better understand mentoring in OSS communities, we analyze the resolution process of 48,402 GFIs from 964 repositories through a mix-method approach. We investigate the extent, the mentorship structures, the discussed topics, and the relevance of expert involvement. We find that ~70\% of GFIs have expert participation, with each GFI usually having one expert who makes two comments. Half of GFIs will receive their first expert comment within 8.5 hours after a newcomer comment. Through analysis of the collaboration networks of newcomers and experts, we observe that community mentorship presents four types of structure: centralized mentoring, decentralized mentoring, collaborative mentoring, and distributed mentoring. As for discussed topics, we identify 14 newcomer challenges and 18 expert mentoring content. By fitting the generalized linear models, we find that expert involvement positively correlates with newcomers’ successful contributions but negatively correlates with newcomers’ retention. Our study manifests the status and significance of mentoring in the OSS projects, which provides rich practical implications for optimizing the mentoring process and helping newcomers contribute smoothly and successfully.
  </p>
</blockquote>

<p>
  Maria Tigina, Anastasiia Birillo, Yaroslav Golubev, Hieke Keuning, Nikolay Vyahhi, and Timofey Bryksin.
  Analyzing the quality of submissions in online programming courses.
  2023.
  <a href="https://arxiv.org/abs/2301.11158">arXiv:2301.11158</a>.
</p>

<blockquote>
  <p>
    Programming education should aim to provide students with a broad range of skills that they will later use while developing software. An important aspect in this is their ability to write code that is not only correct but also of high quality. Unfortunately, this is difficult to control in the setting of a massive open online course. In this paper, we carry out an analysis of the code quality of submissions from JetBrains Academy — a platform for studying programming in an industry-like project-based setting with an embedded code quality assessment tool called Hyperstyle. We analyzed more than a million Java submissions and more than 1.3 million Python submissions, studied the most prevalent types of code quality issues and the dynamics of how students fix them. We provide several case studies of different issues, as well as an analysis of why certain issues remain unfixed even after several attempts. Also, we studied abnormally long sequences of submissions, in which students attempted to fix code quality issues after passing the task. Our results point the way towards the improvement of online courses, such as making sure that the task itself does not incentivize students to write code poorly.
  </p>
</blockquote>

<p>
  Joseph Vargovich, Fabio Santos, Jacob Penney, Marco A. Gerosa, and Igor Steinmacher.
  GiveMeLabeledIssues: an open source issue recommendation system.
  2023.
  <a href="https://arxiv.org/abs/2303.13418">arXiv:2303.13418</a>.
</p>

<blockquote>
  <p>
    Developers often struggle to navigate an Open Source Software (OSS) project’s issue-tracking system and find a suitable task. Proper issue labeling can aid task selection, but current tools are limited to classifying the issues according to their type (e.g., bug, question, good first issue, feature, etc.). In contrast, this paper presents a tool (GiveMeLabeledIssues) that mines project repositories and labels issues based on the skills required to solve them. We leverage the domain of the APIs involved in the solution (e.g., User Interface (UI), Test, Databases (DB), etc.) as a proxy for the required skills. GiveMeLabeledIssues facilitates matching developers’ skills to tasks, reducing the burden on project maintainers. The tool obtained a precision of 83.9\% when predicting the API domains involved in the issues. The replication package contains instructions on executing the tool and including new projects. A demo video is available at <a href="https://www.youtube.com/watch?v=ic2quUue7i8">https://www.youtube.com/watch?v=ic2quUue7i8</a>.
  </p>
</blockquote>
