---
layout: post
author: Greg Wilson
title: "A Few More to close"
date: 2023-04-27
categories: ["Computing Education", "Human Values", "Information Theory", "Machine Learning", "Security", "Software Evolution", "Software Quality", "Types"]
---

<p>
  It will probably be three or four weeks before we can post all of the videos from
  <a href="{{'/2023/04/26/and-thats-a-wrap.html' | relative_url}}">our third set of lightning talks</a>,
  and we all need to catch up with our day jobs in the interim,
  so here are a few more papers that we hope you'll find interesting to tide you over—we'll resume posting
  as the videos come in.
</p>

<p>
  Anastasiia Birillo, Elizaveta Artser, Yaroslav Golubev, Maria Tigina, Hieke Keuning, Nikolay Vyahhi, and Timofey Bryksin.
  Detecting code quality issues in pre-written templates of programming tasks in online courses.
  2023,
  <a href="https://arxiv.org/abs/2304.12376">arXiv:2304.12376</a>.
</p>

<blockquote>
  <p>
    In this work, we developed an algorithm for detecting code quality issues in the templates of online programming tasks, validated it, and conducted an empirical study on the dataset of student solutions. The algorithm consists of analyzing recurring unfixed issues in solutions of different students, matching them with the code of the template, and then filtering the results. Our manual validation on a subset of tasks demonstrated a precision of 80.8% and a recall of 73.3%. We used the algorithm on 415 Java tasks from the JetBrains Academy platform and discovered that as much as 14.7% of tasks have at least one issue in their template, thus making it harder for students to learn good code quality practices. We describe our results in detail, provide several motivating examples and specific cases, and share the feedback of the developers of the platform, who fixed 51 issues based on the output of our approach.
  </p>
</blockquote>

<p>
  Louis F. DeKoven, Audrey Randall, Ariana Mirian, Gautam Akiwate, Ansel Blume, Lawrence K. Saul, Aaron Schulman, Geoffrey M. Voelker, and Stefan Savage.
  Measuring security practices and how they impact security.
  In <em>Proceedings of the Internet Measurement Conference</em>. ACM, Oct 2019,
  <a href="https://doi.org/10.1145/3355369.3355571">doi:10.1145/3355369.3355571</a>.
</p>

<blockquote>
  <p>
    Security is a discipline that places significant expectations on lay users. Thus, there are a wide array of technologies and behaviors that we exhort end users to adopt and thereby reduce their security risk. However, the adoption of these “best practices” — ranging from the use of antivirus products to actively keeping software updated — is not well understood, nor is their practical impact on security risk well-established. This paper explores both of these issues via a large-scale empirical measurement study covering approximately 15,000 computers over six months. We use passive monitoring to infer and characterize the prevalence of various security practices in situ as well as a range of other potentially security-relevant behaviors. We then explore the extent to which differences in key security behaviors impact real-world outcomes (i.e., that a device shows clear evidence of having been compromised).
  </p>
</blockquote>

<p>
  Malinda Dilhara, Danny Dig, and Ameya Ketkar.
  PYEVOLVE: automating frequent code changes in Python ML systems.
  <a href="https://danny.cs.colorado.edu/papers/PyEvolve_ICSE2023.pdf">https://danny.cs.colorado.edu/papers/PyEvolve_ICSE2023.pdf</a>, 2023.
</p>

<blockquote>
  <p>
    Because of the naturalness of software and the rapid evolution of Machine Learning (ML) techniques, frequently repeated code change patterns (CPATs) occur often. They range from simple API migrations to changes involving several complex control structures such as for loops. While manually performing CPATs is tedious, the current state-of-the-art techniques for inferring transformation rules are not advanced enough to handle unseen variants of complex CPATs, resulting in a low recall rate. In this paper we present a novel, automated workflow that mines CPATs, infers the transformation rules, and then transplants them automatically to new target sites. We designed, implemented, evaluated and released this in a tool, PYEVOLVE. At its core is a novel data-flow, control-flow aware transformation rule inference engine. Our technique allows us to advance the state-of-the-art for transformation-by-example tools; without it, 70% of the code changes that PYEVOLVE transforms would not be possible to automate. Our thorough empirical evaluation of over 40,000 transformations shows 97% precision and 94% recall. By accepting 90% of CPATs generated by PYEVOLVE in famous open-source projects, developers confirmed its changes are useful.
  </p>
</blockquote>

<p>
  Maria Angela Ferrario and Emily Winter.
  Applying human values theory to software engineering practice: lessons and implications.
  <em>IEEE Transactions on Software Engineering</em>, 49(3):973–990, Mar 2023,
  <a href="https://doi.org/10.1109/tse.2022.3170087">doi:10.1109/tse.2022.3170087</a>.
</p>

<blockquote>
  <p>
    The study of human values in software engineering (SE) is increasingly recognized as a fundamental human-centric issue of SE decision making. However, values studies in SE still face a number of issues, including the difficulty of eliciting values in a systematic and structured way, the challenges of measuring and tracking values over time, and the lack of practice-based understanding of values among software practitioners. This paper aims to help address these issues by: 1) outlining a research framework that supports a systematic approach to values elicitation, analysis, and understanding; 2) introducing tools and techniques that help elicit and measure values during SE decision making processes in a systematic way; and 3) applying such tools to a month-long research sprint co-designed with an industry partner and conducted with 27 software practitioners. The case study builds on lessons from an earlier pilot (12 participants) and combines in-situ observations with the use of two values-informed tools: the Values Q-Sort (V-QS), and the Values-Retro. The V-QS adapts instruments from values research to the SE context, the Values-Retro adapts existing SE techniques to values theory. We distil implications for research and practice in ten lessons learned.
  </p>
</blockquote>

<p>
  Catarina Gamboa, Paulo Alexandre Santos, Christopher S. Timperley, and Alcides Fonseca.
  User-driven design and evaluation of liquid types in java.
  2021.
  <a href="https://arxiv.org/abs/arXiv:2110.05444">arXiv:arXiv:2110.05444</a>.
</p>

<blockquote>
  <p>
    Bugs that are detected earlier during the development lifecycle are easier and cheaper to fix, whereas bugs that are found during production are difficult and expensive to address, and may have dire consequences. Type systems are particularly effective at identifying and preventing bugs early in the development lifecycle by causing invalid programs to result in build failure. Liquid Types are more powerful than those found in mainstream programming languages, allowing the detection of more classes of bugs. However, while Liquid Types were proposed in 2008 with their integration in ML and subsequently introduced in C (2012), Javascript(2012) and Haskell(2014) through language extensions, they have yet to become widely adopted by mainstream developers. This paper investigates how Liquid Types can be integrated in a mainstream programming language, Java, by proposing a new design that aims to lower the barrier to entry and adapts to problems that Java developers commonly encounter at runtime. To promote accessibility, we conducted a series of developer surveys to design the syntax of LiquidJava, our prototype. To evaluate the prototype's usability, we conducted a user study of 30 Java developers, concluding that users intend to use LiquidJava and that it helped to find more bugs and debug faster.
  </p>
</blockquote>

<p>
  Hieke Keuning, Johan Jeuring, and Bastiaan Heeren.
  A systematic mapping study of code quality in education – with complete bibliography.
  2023,
  <a href="https://arxiv.org/abs/arXiv:2304.13451">arXiv:arXiv:2304.13451</a>.
</p>

<blockquote>
  <p>
    While functionality and correctness of code has traditionally been the main focus of computing educators, quality aspects of code are getting increasingly more attention. High-quality code contributes to the maintainability of software systems, and should therefore be a central aspect of computing education. We have conducted a systematic mapping study to give a broad overview of the research conducted in the field of code quality in an educational context. The study investigates paper characteristics, topics, research methods, and the targeted programming languages. We found 195 publications (1976–2022) on the topic in multiple databases, which we systematically coded to answer the research questions. This paper reports on the results and identifies developments, trends, and new opportunities for research in the field of code quality in computing education.
  </p>
</blockquote>

<p>
  Ifeanyi G. Ndukwe, Sherlock A. Licorish, Amjed Tahir, and Stephen G. MacDonell.
  How have views on software quality differed over time? research and practice viewpoints.
  <em>Journal of Systems and Software</em>, 195:111524, Jan 2023,
  <a href="https://doi.org/10.1016/j.jss.2022.111524">doi:10.1016/j.jss.2022.111524</a>.
</p>

<blockquote>
  <p>
    Context: Over the years, there has been debate about what constitutes software quality and how it should be measured. This controversy has caused uncertainty across the software engineering community, affecting levels of commitment to the many potential determinants of quality among developers. An up-to-date catalogue of software quality views could provide developers with contem- porary guidelines and templates. In fact, it is necessary to learn about views on the quality of code on frequently used online collaboration platforms (e.g., Stack Overflow), given that the quality of code snippets can affect the quality of software products developed. If quality models are unsuitable for aiding developers because they lack relevance, developers will hold relaxed or inappropriate views of software quality, thereby lacking awareness and commitment to such practices.
  </p>
  <p>
    Objective: We aim to explore differences in interest in quality characteristics across research and practice. We also seek to identify quality characteristics practitioners consider important when judging code snippet quality. First, we examine the literature for quality characteristics used frequently for judging software quality, followed by the quality characteristics commonly used by researchers to study code snippet quality. Finally, we investigate quality characteristics used by practitioners to judge the quality of code snippets.
  </p>
  <p>
    Methods: We conducted two systematic literature reviews followed by semi-structured interviews of 50 practitioners to address this gap.
  </p>
  <p>
    Results: The outcomes of the semi-structured interviews revealed that most practitioners judged the quality of code snippets using five quality dimensions: Functionality, Readability, Efficiency, Security and Reliability. However, other dimensions were also considered (i.e., Reusability, Maintainability, Usability, Compatibility and Completeness). This outcome differed from how the researchers judged code snippet quality.
  </p>
  <p>
    Conclusion: Practitioners today mainly rely on code snippets from online code resources, and specific models or quality characteristics are emphasised based on their need to address distinct concerns (e.g., mobile vs web vs standalone applications, regular vs machine learning applications, or open vs closed source applications). Consequently, software quality models should be adapted for the domain of consideration and not seen as one-size-fits-all. This study will lead to targeted support for various clusters of the software development community.
  </p>
</blockquote>

<p>
  Adriano Torres, Sebastian Baltes, Christoph Treude, and Markus Wagner.
  Applying information theory to software evolution.
  2023,
  <a href="https://arxiv.org/abs/2303.13729">arXiv:2303.13729</a>.
</p>

<blockquote>
  <p>
    Although information theory has found success in disciplines, the literature on its applications to software evolution is limit. We are still missing artifacts that leverage the data and tooling available to measure how the information content of a project can be a proxy for its complexity. In this work, we explore two definitions of entropy, one structural and one textual, and apply it to the historical progression of the commit history of 25 open source projects. We produce evidence that they generally are highly correlated. We also observed that they display weak and unstable correlations with other complexity metrics. Our preliminary investigation of outliers shows an unexpected high frequency of events where there is considerable change in the information content of the project, suggesting that such outliers may inform a definition of surprisal.
  </p>
</blockquote>
