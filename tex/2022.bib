@inproceedings{AbouKhalil2022,
  doi = {10.1145/3524842.3528494},
  title = {The general index of software engineering papers},
  booktitle = msr,
  author = {{Abou Khalil}, Zeinab and Zacchiroli, Stefano},
  publisher = {ACM},
  month = may,
  year = {2022}
  abstract = {We introduce the General Index of Software Engineering Papers, a dataset of fulltext-indexed papers from the most prominent scientific venues in the field of Software Engineering. The dataset includes both complete bibliographic information and indexed n-grams (sequence of contiguous words after removal of stopwords and non-words, for a total of 577 276 382 unique n-grams in this release) with length 1 to 5 for 44 581 papers retrieved from 34 venues over the 1971–2020 period. The dataset serves use cases in the field of meta-research, allowing to introspect the output of software engineering research even when access to papers or scholarly search engines is not possible (e.g., due to contractual reasons). The dataset also contributes to making such analyses reproducible and independently verifiable, as opposed to what happens when they are conducted using 3rd-party and non-open scholarly indexing services. The dataset is available as a portable Postgres database dump and released as open data.}
}

@inproceedings{Ait2022,
  doi = {10.1145/3524842.3527941},
  title = {An empirical study on the survival rate of {GitHub} projects},
  booktitle = msr,
  author = {Ait, Adem and Izquierdo, Javier Luis C{\'a}novas and Cabot, Jordi},
  publisher = {ACM},
  month = may,
  year = {2022},
  abstract = {The number of Open Source projects hosted in social coding platforms such as GitHub is constantly growing. However, many of these projects are not regularly maintained and some are even abandoned shortly after they were created. In this paper we analyze early project development dynamics in software projects hosted on GitHub, including their survival rate. To this aim, we collected all 1,127 GitHub repositories from four different ecosystems (i.e., NPM packages, R packages, WordPress plugins and Laravel packages) created in 2016. We stored their activity in a time series database and analyzed their activity evolution along their lifespan, from 2016 to now. Our results reveal that the prototypical development process consists of intensive coding-driven active periods followed by long periods of inactivity. More importantly, we have found that a significant number of projects die in the first year of existence with the survival rate decreasing year after year. In fact, the probability of surviving longer than five years is less than 50\% though some types of projects have better chances of survival.}
}

@article{Baxter2022,
  doi = {10.1002/spe.3120},
  title = {Collaborative experience between scientific software projects using Agile Scrum development},
  author = {Baxter, Amanda L and BenZvi, Segev Y and Bonivento, Walter and Brazier, Adam and Clark, Michael and Coleiro, Alexis and Collom, David and Colomer-Molla, Marta and Cousins, Bryce and Delgado Orellana, Aliwen and Dornic, Damien and Ekimtcov, Vladislav and ElSayed, Shereen and Gallo Rosso, Andrea and Godwin, Patrick and Griswold, Spencer and Habig, Alec and Hill, Remington and Horiuchi, Shunsaku and Howell, D Andrew and Johnson, Margaret W G and Juri{\'c}, Mario and Kneller, James P and Kopec, Abigail and Kopper, Claudio and Kulikovskiy, Vladimir and Lamoureux, Mathieu and Lang, Rafael F and Li, Shengchao and Lincetto, Massimiliano and Lindstrom, Lindy and Linvill, Mark W and McCully, Curtis and Migenda, Jost and Milisavljevic, Danny and Nelson, Spencer and Novoseltseva, Rita and O'Sullivan, Erin and Petravick, Donald and Pointon, Barry W and Raj, Nirmal and Renshaw, Andrew and Rumleskie, Janet and Sonley, Tom and Tapia, Ron and Tseng, Jeffrey C L and Tunnell, Christopher D and Vannoye, Godefroy and Vigorito, Carlo F and Virtue, Clarence J and Weaver, Christopher and Weil, Kathryn E and Winslow, Lindley and Wolski, Rich and Xu, Xun- Jie and Xu, Yiyang and {The SCiMMA and SNEWS Collaborations}},
  journal = {Softw. Pract. Exp.},
  publisher = {Wiley},
  volume = {52},
  number = {10},
  pages = {2077--2096},
  month = oct,
  year = {2022},
  copyright = {http://creativecommons.org/licenses/by-nc/4.0/},
  abstract = {Developing sustainable software for the scientific community requires expertise in software engineering and domain science. This can be challenging due to the unique needs of scientific software, the insufficient resources for software engineering practices in the scientific community, and the complexity of developing for evolving scientific contexts. While open‐source software can partially address these concerns, it can introduce complicating dependencies and delay development. These issues can be reduced if scientists and software developers collaborate. We present a case study wherein scientists from the SuperNova Early Warning System collaborated with software developers from the Scalable Cyberinfrastructure for Multi‐Messenger Astrophysics project. The collaboration addressed the difficulties of open‐source software development, but presented additional risks to each team. For the scientists, there was a concern of relying on external systems and lacking control in the development process. For the developers, there was a risk in supporting a user‐group while maintaining core development. These issues were mitigated by creating a second Agile Scrum framework in parallel with the developers' ongoing Agile Scrum process. This Agile collaboration promoted communication, ensured that the scientists had an active role in development, and allowed the developers to evaluate and implement the scientists' software requirements. The collaboration provided benefits for each group: the scientists actuated their development by using an existing platform, and the developers utilized the scientists' use‐case to improve their systems. This case study suggests that scientists and software developers can avoid scientific computing issues by collaborating and that Agile Scrum methods can address emergent concerns.}
}

@inproceedings{Beasley2022,
  doi = {10.1145/3502718.3524772},
  title = {The impact of remote pair programming in an upper-level {CS} course},
  booktitle = iticse,
  author = {Beasley, Zachariah J and Johnson, Ayesha R},
  publisher = {ACM},
  month = jul,
  year = {2022},
  conference = iticse,
  abstract = {Pair programming is an active learning technique with several benefits to students, including increasing participation and improving outcomes, particularly for female computer science students. However, most of the literature highlights the effects of pair programming in introductory courses, where students have different prior programming experience and thus may experience group issues. This work analyzes the effect of pair programming in an upper-level computer science course, where students have a more consistent background education, particularly in languages learned and coding best practices. Secondly, the effect of remote pair programming on student outcomes is still an open question of increasing importance with the advent of Covid-19. This work utilized split sections with a control and treatment group in a large, public university. In addition to comparing pair programming to individual programming, results were analyzed by modality (remote vs. in person) and by gender, focusing on how pair programming benefits female computer science students in confidence, persistence in the major, and outcomes. We found that pair programming groups scored higher on assignments and exams, that remote pair programming groups performed as well as in person groups, and that female students increased their confidence in asking questions in class and scored 12\% higher in the course when utilizing pair programming.}
}

@inproceedings{Bendrissou2022,
  doi = {10.1145/3519939.3523716},
  title = {``Synthesizing input grammars'': a replication study},
  booktitle = pldi,
  author = {Bendrissou, Bachir and Gopinath, Rahul and Zeller, Andreas},
  publisher = {ACM},
  month = jun,
  year = {2022},
  conference = pldi,
  abstract = {When producing test inputs for a program, test generators (\"fuzzers\") can greatly profit from grammars that formally describe the language of expected inputs. In recent years, researchers thus have studied means to recover input grammars from programs and their executions. The GLADE algorithm by Bastani et al., published at PLDI 2017, was the first black-box approach to claim context-free approximation of input specification for non-trivial languages such as XML, Lisp, URLs, and more. Prompted by recent observations that the GLADE algorithm may show lower performance than reported in the original paper, we have reimplemented the GLADE algorithm from scratch. Our evaluation confirms that the effectiveness score (F1) reported in the GLADE paper is overly optimistic, and in some cases, based on the wrong language. Furthermore, GLADE fares poorly in several real-world languages evaluated, producing grammars that spend megabytes to enumerate inputs.}
}

@article{Bijlsma2022,
  doi = {10.1002/spe.3061},
  title = {Evaluation of design pattern alternatives in Java},
  author = {Bijlsma, Lex A and Kok, Arjan J F and Passier, Harrie J M and Pootjes, Harold J and Stuurman, Sylvia},
  journal = {Softw. Pract. Exp.},
  publisher = {Wiley},
  volume = {52},
  number = {5},
  pages = {1305--1315},
  month = may,
  year = {2022},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
  abstract = {Design patterns are standard solutions to common design problems. The famous Gang of Four book describes more than twenty design patterns for the object‐oriented paradigm. These patterns were developed more than twenty‐five years ago, using the programming language concepts available at that time. Patterns do not always fit underlying domain concepts. For example, even when a concrete strategy is a pure function, the classical strategy pattern represents this as a separate subclass and as such obscures the intent of this pattern with extra complexities due to the inheritance‐based implementation. Due to the ongoing development of oo‐languages, a relevant question is whether the implementation of these patterns can be improved using new language features, such that they fit more closely with the intent. An additional question is then how we can decide which implementation is to be preferred. In this article, we investigate both questions, using the strategy pattern as an example. Our main contribution is that we show how to reason about different implementations, using both the description of a design pattern and design principles as guidance.}
}

@inproceedings{Bittner2022,
  doi = {10.1145/3540250.3549108},
  title = {Classifying edits to variability in source code},
  booktitle = esec-fse,
  author = {Bittner, Paul Maximilian and Tinnes, Christof and Schulthei{\ss}, Alexander and Viegener, S{\"o}ren and Kehrer, Timo and Th{\"u}m, Thomas},
  publisher = {ACM},
  month = nov,
  year = {2022},
  conference = esec-fse,
  abstract = {For highly configurable software systems, such as the Linux kernel, maintaining and evolving variability information along changes to source code poses a major challenge. While source code itself may be edited, also feature-to-code mappings may be introduced, removed, or changed. In practice, such edits are often conducted ad-hoc and without proper documentation. To support the maintenance and evolution of variability, it is desirable to understand the impact of each edit on the variability. We propose the first complete and unambiguous classification of edits to variability in source code by means of a catalog of edit classes. This catalog is based on a scheme that can be used to build classifications that are complete and unambiguous by construction. To this end, we introduce a complete and sound model for edits to variability. In about 21.5ms per commit, we validate the correctness and suitability of our classification by classifying each edit in 1.7 million commits in the change histories of 44 open-source software systems automatically. We are able to classify all edits with syntactically correct feature-to-code mappings and find that all our edit classes occur in practice.}
}

@inproceedings{Boag2022,
  doi = {10.1145/3531146.3533111},
  title = {Tech worker organizing for power and accountability},
  booktitle = {2022 {ACM} Conference on Fairness, Accountability, and Transparency},
  author = {Boag, William and Suresh, Harini and Lepe, Bianca and D'Ignazio, Catherine},
  publisher = {ACM},
  month = jun,
  year = {2022},
  conference = {FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency},
  abstract = {In recent years, there has been a growing interest in the field of “AI Ethics” and related areas. This field is purposefully broad, allowing for the intersection of numerous subfields and disciplines. However, a lot of work in this area thus far has centered computational methods, leading to a narrow lens where technical tools are framed as solutions for broader sociotechnical problems. In this work, we discuss a less-explored mode of what it can mean to “do” AI Ethics: tech worker collective action. Through collective action, the employees of powerful tech companies can act as a countervailing force against strong corporate impulses to grow or make a profit to the detriment of other values. In this work, we ground these efforts in existing scholarship of social movements and labor organizing. We characterize 150 documented collective actions, and explore several case studies of successful campaigns. Looking forward, we also identify under-explored types of actions, and provide conceptual frameworks and inspiration for how to utilize worker organizing as an effective lever for change.}
}

@article{Britton2017,
  doi = {10.1080/02602938.2015.1116497},
  title = {Assessing teamwork in undergraduate education: a measurement tool to evaluate individual teamwork skills},
  author = {Britton, Emily and Simper, Natalie and Leger, Andrew and Stephenson, Jenn},
  journal = {Assess. Eval. High. Educ.},
  publisher = {Informa UK Limited},
  volume = {42},
  number = {3},
  pages = {378--397},
  month = apr,
  year = {2017},
  abstract = {Effective teamwork skills are essential for success in an increasingly team-based workplace. However, research suggests that there is often confusion concerning how teamwork is measured and assessed, making it difficult to develop these skills in undergraduate curricula. The goal of the present study was to develop a sustainable tool for assessing individual teamwork skills, with the intention of refining and measuring these skills over time. The TeamUp rubric was selected as the preliminary standardised measure of teamwork and tested in a second year undergraduate course (Phase One). Although the tool displayed acceptable psychometric properties, there was concern that it was too lengthy, compromising student completion. This prompted refinement and modification leading to the development of the Team-Q, which was again tested in the same undergraduate course (Phase Two). The new tool had high internal consistency, as well as conceptual similarity to other measures of teamwork. Estimates of inter-rater reliability were within a satisfactory range, although it was determined that logistical issues limited the feasibility of external evaluations. Preliminary evidence suggests that teamwork skills improve over time when taught and assessed, providing support for the continued application of the Team-Q as a tool for developing teamwork skills in undergraduate education.}
}

@inproceedings{Brodley2022,
  doi = {10.1145/3478431.3499352},
  title = {Broadening participation in computing via ubiquitous combined majors ({CS+X})},
  booktitle = sigcse,
  author = {Brodley, Carla E and Hescott, Benjamin J and Biron, Jessica and Ressing, Ali and Peiken, Melissa and Maravetz, Sarah and Mislove, Alan},
  publisher = {ACM},
  month = feb,
  year = {2022},
  conference = sigcse,
  abstract = {In 2001, Khoury College of Computer Sciences at Northeastern University created their first combined majors with Cognitive Psychology, Mathematics and Physics. This type of degree has often been referred to as \"CS+X\" in the literature and is increasingly relevant as the need for interdisciplinary computer scientists grows. As of 2021, students at Northeastern can choose among three computing majors (Computer Science, Data Science or Cybersecurity) and 42 combined majors, which combine one of the three computing degrees with one of 29 distinct majors in other fields. Prior to 2014, combined majors were with the sciences, business and design. Over the last seven years, we created 29 new combined majors, explicitly creating combinations with fields where there has traditionally been greater gender diversity. The resulting increase in student interest and gender diversity over the last seven years is compelling. As of Fall 2020, 44.6\% of the 2,800+ computing majors at Northeastern are pursuing combined majors, 39\% of whom are women. This is substantially higher than the 21.5\% reported in IPEDS for 2019 women computing graduates in the U.S. We did not observe any significant differences in racial and ethnic diversity between combined and computing only degrees. In this experience paper, we describe how we create and manage combined majors, and we present results on enrollments, admissions, graduation, internship placements, and how students discover combined majors.}
}

@inproceedings{Buffardi2020,
  doi = {10.1145/3328778.3366948},
  title = {Assessing individual contributions to software engineering projects with {Git} logs and user stories},
  booktitle = sigcse,
  author = {Buffardi, Kevin},
  publisher = {ACM},
  month = feb,
  year = {2020},
  conference = sigcse,
  abstract = {Software Engineering courses often incorporate large-scale projects with collaboration between students working in teams. However, it is difficult to objectively assess individual students when their projects are a product of collaborative efforts. This study explores measurements of individuals' contributions to their respective teams. I analyzed ten Software Engineering team projects (n=42) and evaluations of individual contributions using automated evaluation of the version control system history (Git logs) and user stories completed on their project management (Kanban) boards. Unique insights from meta-data within the Git history and Kanban board user stories reveal complicated relationships between these measurements and traditional assessments, such as peer review and subjective instructor evaluation. From the results, I suggest supplementing and validating traditional assessments with insights from individuals' commit history and user story contributions.}
}

@inproceedings{Collaris2022,
  doi = {10.1145/3546155.3546670},
  title = {Characterizing data scientists' mental models of local feature importance},
  booktitle = {Nordic {Human-Computer} Interaction Conference},
  author = {Collaris, Dennis and Weerts, Hilde J P and Miedema, Daphne and van Wijk, Jarke J and Pechenizkiy, Mykola},
  publisher = {ACM},
  month = oct,
  year = {2022},
  conference = {NordiCHI '22: Nordic Human-Computer Interaction Conference},
  abstract = {Feature importance is an approach that helps to explain machine learning model predictions. It works through assigning importance scores to input features of a particular model. Different techniques exist to derive these scores, with widely varying underlying assumptions of what importance means. Little research has been done to verify whether these assumptions match the expectations of the target user, which is imperative to ensure that feature importance values are not misinterpreted. In this work, we explore data scientists’ mental models of (local) feature importance and compare these with the conceptual models of the techniques. We first identify several properties of local feature importance techniques that could potentially lead to misinterpretations. Subsequently, we explore the expectations data scientists have about local feature importance through an exploratory (qualitative and quantitative) survey of 34 data scientists in industry. We compare the identified expectations to the theory and assumptions behind the techniques and find that the two are not (always) in agreement.}
}

@inproceedings{DalSasso2016,
  doi = {10.1109/QRS.2016.28},
  title = {What Makes a Satisficing Bug Report?},
  booktitle = {2016 {IEEE} International Conference on Software Quality, Reliability and Security ({QRS})},
  author = {{Dal Sasso}, Tommaso and Mocci, Andrea and Lanza, Michele},
  publisher = {IEEE},
  month = aug,
  year = {2016},
  conference = {2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)},
  abstract = {To ensure quality of software systems, developers use bug reports to track defects. It is in the interest of users and developers that bug reports provide the necessary information to ease the fixing process. Past research found that users do not provide the information that developers deem ideally useful to fix a bug. This raises an interesting question: What is the satisficing information to speed up the bug fixing process? We conducted an observational study on the relation between provided report information and its lifetime, considering more than 650,000 reports from open-source systems using popular bug trackers. We distilled a meta-model for a minimal bug report, establishing a basic layer of core features. We found that few fields influence the resolution time and that customized fields have little impact on it. We performed a survey to investigate what users deem easy to provide in a bug report.}
}

@inproceedings{DiGrazia2022,
  doi = {10.1145/3540250.3549114},
  title = {The evolution of type annotations in {Python}: an empirical study},
  booktitle = esec-fse,
  author = {{Di Grazia}, Luca and Pradel, Michael},
  publisher = {ACM},
  month = nov,
  year = {2022},
  conference = esec-fse,
  abstract = {Type annotations and gradual type checkers attempt to reveal errors and facilitate maintenance in dynamically typed programming languages. Despite the availability of these features and tools, it is currently unclear how quickly developers are adopting them, what strategies they follow when doing so, and whether adding type annotations reveals more type errors. This paper presents the first large-scale empirical study of the evolution of type annotations and type errors in Python. The study is based on an analysis of 1,414,936 type annotation changes, which we extract from 1,123,393 commits among 9,655 projects. Our results show that (i) type annotations are getting more popular, and once added, often remain unchanged in the projects for a long time, (ii) projects follow three evolution patterns for type annotation usage -- regular annotation, type sprints, and occasional uses -- and that the used pattern correlates with the number of contributors, (iii) more type annotations help find more type errors (0.704 correlation), but nevertheless, many commits (78.3\%) are committed despite having such errors. Our findings show that better developer training and automated techniques for adding type annotations are needed, as most code still remains unannotated, and they call for a better integration of gradual type checking into the development process.}
}

@article{Dogan2022,
  doi = {10.1016/j.infsof.2021.106737},
  title = {Towards a taxonomy of code review smells},
  author = {Do{\u g}an, Emre and T{\"u}z{\"u}n, Eray},
  journal = {Inf. Softw. Technol.},
  publisher = {Elsevier BV},
  volume = {142},
  number = {106737},
  pages = {106737},
  month = feb,
  year = {2022},
  abstract = {}
}

@article{Drage2022,
  doi = {10.1007/s13347-022-00543-1},
  title = {Does {AI} debias recruitment? Race, gender, and {AI's} ``eradication of difference''},
  author = {Drage, Eleanor and Mackereth, Kerry},
  abstract = {In this paper, we analyze two key claims offered by recruitment AI companies in relation to the development and deployment of AI-powered HR tools: (1) recruitment AI can objectively assess candidates by removing gender and race from their systems, and (2) this removal of gender and race will make recruitment fairer, help customers attain their DEI goals, and lay the foundations for a truly meritocratic culture to thrive within an organization. We argue that these claims are misleading for four reasons: First, attempts to ``strip'' gender and race from AI systems often misunderstand what gender and race are, casting them as isolatable attributes rather than broader systems of power. Second, the attempted outsourcing of ``diversity work'' to AI-powered hiring tools may unintentionally entrench cultures of problems within organizations. Third, AI hiring tools' supposedly neutral assessment of candidates' traits belie the power relationship between the observer and the observed. Specifically, the racialized history of character analysis and its associated processes of classification and categorization play into longer histories of taxonomical sorting and reflect the current demands and desires of the job market, even when not explicitly conducted along the lines of gender and race. Fourth, recruitment AI tools help produce the ``ideal candidate'' that they supposedly identify through by constructing associations between words and people's bodies. From these four conclusions outlined above, we offer three key recommendations to AI HR firms, their customers, and policy makers going forward.},
  journal = {Philos. Technol.},
  volume = {35},
  number = {4},
  pages = {89},
  month = oct,
  year = {2022},
  keywords = {Artificial intelligence; Bias; Gender; Hiring; Race; Recruitment}
}

@inproceedings{Dyer2022,
  doi = {10.1145/3540250.3549158},
  title = {An exploratory study on the predominant programming paradigms in {Python} code},
  booktitle = esec-fse,
  author = {Dyer, Robert and Chauhan, Jigyasa},
  publisher = {ACM},
  month = nov,
  year = {2022},
  conference = esec-fse,
  abstract = {Python is a multi-paradigm programming language that fully supports object-oriented (OO) programming. The language allows writing code in a non-procedural imperative manner, using procedures, using classes, or in a functional style. To date, no one has studied what paradigm(s), if any, are predominant in Python code and projects. In this work, we first define a technique to classify Python files into predominant paradigm(s). We then automate our approach and evaluate it against human judgements, showing over 80\% agreement. We then analyze over 100k open-source Python projects, automatically classifying each source file and investigating the paradigm distributions. The results indicate Python developers tend to heavily favor OO features. We also observed a positive correlation between OO and procedural paradigms and the size of the project. And despite few files or projects being predominantly functional, we still found many functional feature uses.}
}

@article{Flyvbjerg2022,
  doi = {10.1080/07421222.2022.2096544},
  title = {The empirical reality of {IT} project cost overruns: Discovering A power-law distribution},
  author = {Flyvbjerg, Bent and Budzier, Alexander and Lee, Jong Seok and Keil, Mark and Lunn, Daniel and Bester, Dirk W},
  journal = {J. Manag. Inf. Syst.},
  publisher = {Informa UK Limited},
  volume = {39},
  number = {3},
  pages = {607--639},
  month = jul,
  year = {2022},
  abstract = {ABSTRACT If managers assume a normal or near-normal distribution of Information Technology (IT) project cost overruns, as is common, and cost overruns can be shown to follow a power-law distribution, managers may be unwittingly exposing their organizations to extreme risk by severely underestimating the probability of large cost overruns. In this research, we collect and analyze a large sample comprised of 5,392 IT projects to empirically examine the probability distribution of IT project cost overruns. Further, we propose and examine a mechanism that can explain such a distribution. Our results reveal that IT projects are far riskier in terms of cost than normally assumed by decision makers and scholars. Specifically, we found that IT project cost overruns follow a power-law distribution in which there are a large number of projects with relatively small overruns and a fat tail that includes a smaller number of projects with extreme overruns. A possible generative mechanism for the identified power-law distribution is found in interdependencies among technological components in IT systems. We propose and demonstrate, through computer simulation, that a problem in a single technological component can lead to chain reactions in which other interdependent components are affected, causing substantial overruns. What the power law tells us is that extreme IT project cost overruns will occur and that the prevalence of these will be grossly underestimated if managers assume that overruns follow a normal or near-normal distribution. This underscores the importance of realistically assessing and mitigating the cost risk of new IT projects up front.}
}

@inproceedings{Foidl2022,
  doi = {10.1145/3522664.3528590},
  title = {Data smells},
  booktitle = {Proceedings of the 1st International Conference on {AI} Engineering: Software Engineering for {AI}},
  author = {Foidl, Harald and Felderer, Michael and Ramler, Rudolf},
  publisher = {ACM},
  month = may,
  year = {2022},
  conference = {CAIN '22: 1st Conference on AI Engineering - Software Engineering for AI},
  abstract = {High data quality is fundamental for today’s AI-based systems. However, although data quality has been an object of research for decades, there is a clear lack of research on potential data quality issues (e.g., ambiguous, extraneous values). These kinds of issues are latent in nature and thus often not obvious. Nevertheless, they can be associated with an increased risk of future problems in AI-based systems (e.g., technical debt, data-induced faults). As a counterpart to code smells in software engineering, we refer to such issues as Data Smells. This article conceptualizes data smells and elaborates on their causes, consequences, detection, and use in the context of AI-based systems. In addition, a catalogue of 36 data smells divided into three categories (i.e., Believability Smells, Understandability Smells, Consistency Smells) is presented. Moreover, the article outlines tool support for detecting data smells and presents the result of an initial smell detection on more than 240 real-world datasets.}
}

@inproceedings{Friend2022,
  doi = {10.1145/3564721.3564738},
  title = {Solve this! {K-12} {CS} education teachers' problems of practice},
  booktitle = {Koli Calling '22: 22nd Koli Calling International Conference on Computing Education Research},
  author = {Friend, Michelle and Mcgill, Monica and Reinking, Anni},
  publisher = {ACM},
  month = nov,
  year = {2022},
  conference = {Koli 2022: 22nd Koli Calling International Conference on Computing Education Research},
  abstract = {Problem. Educational research identifies answerable questions, but often does not address the problems K-12 teachers identify as important. Further, academic research findings can be difficult for teachers to apply to their practices and unique contexts. Currently, little research exists on the lived experiences of primary and secondary instructors who teach computer science (CS) or computational thinking (CT) and also on the specific problems of practice teachers face when teaching CS. Research Question. What problems of practice do K-12 teachers face when teaching CS/CT? Method. Data for this qualitative study was collected using an online questionnaire distributed to teachers internationally. CS/CT teachers responded to an open-ended prompt asking for problems related to teaching CS. The data was analyzed using descriptive first-round coding and focused second-round coding. Validity was established through collaborative coding. Analysis was theorized using locus of control. Findings. Problems with students encompassed behavioral, cognitive, and attitudinal issues, as well as lack of home support or resources. Teachers identified many problems of policy notably stemming from lack of resources or support from administrators. A smaller number of challenges, such as lack of content knowledge, were situated within teachers themselves. While some problems such as student motivation are general, a number of responses identified unique challenges in CS education compared to other disciplines. Implications. Identifying problems faced by teachers can guide professional development offerings, help researchers develop studies that would result in meaningful improvement to CS education, and suggest policy decisions which would result in better outcomes for students.}
}

@article{Gaffney2022,
  doi = {10.14778/3554821.3554842},
  title = {{SQLite}},
  author = {Gaffney, Kevin P and Prammer, Martin and Brasfield, Larry and Hipp, D Richard and Kennedy, Dan and Patel, Jignesh M},
  abstract = {In the two decades following its initial release, SQLite has become the most widely deployed database engine in existence. Today, SQLite is found in nearly every smartphone, computer, web browser, television, and automobile. Several factors are likely responsible for its ubiquity, including its in-process design, standalone codebase, extensive test suite, and cross-platform file format. While it supports complex analytical queries, SQLite is primarily designed for fast online transaction processing (OLTP), employing row-oriented execution and a B-tree storage format. However, fueled by the rise of edge computing and data science, there is a growing need for efficient in-process online analytical processing (OLAP). DuckDB, a database engine nicknamed ``the SQLite for analytics'', has recently emerged to meet this demand. While DuckDB has shown strong performance on OLAP benchmarks, it is unclear how SQLite compares. Furthermore, we are aware of no work that attempts to identify root causes for SQLite's performance behavior on OLAP workloads. In this paper, we discuss SQLite in the context of this changing workload landscape. We describe how SQLite evolved from its humble beginnings to the full-featured database engine it is today. We evaluate the performance of modern SQLite on three benchmarks, each representing a different flavor of in-process data management, including transactional, analytical, and blob processing. We delve into analytical data processing on SQLite, identifying key bottlenecks and weighing potential solutions. As a result of our optimizations, SQLite is now up to 4.2X faster on SSB. Finally, we discuss the future of SQLite, envisioning how it will evolve to meet new demands and challenges.},
  journal = {Proceedings VLDB Endowment},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {15},
  number = {12},
  pages = {3535--3547},
  month = aug,
  year = {2022}
}

@inproceedings{Galappaththi2022,
  doi = {10.1145/3524842.3528435},
  title = {Does this apply to me?},
  booktitle = msr,
  author = {Galappaththi, Akalanka and Nadi, Sarah and Treude, Christoph},
  publisher = {ACM},
  month = may,
  year = {2022},
  abstract = {Stack Overflow has become an essential technical resource for developers. However, given the vast amount of knowledge available on Stack Overflow, finding the right information that is relevant for a given task is still challenging, especially when a developer is looking for a solution that applies to their specific requirements or technology stack. Clearly marking answers with their technical context, i.e., the information that characterizes the technologies and assumptions needed for this answer, is potentially one way to improve navigation. However, there is no information about how often such context is mentioned, and what kind of information it might offer. In this paper, we conduct an empirical study to understand the occurrence of technical context in Stack Overflow answers and comments, using tags as a proxy for technical context. We specifically focus on additional context, where answers/comments mention information that is not already discussed in the question. Our results show that nearly half of our studied threads contain at least one additional context. We find that almost 50\% of the additional context are either a library/framework, a programming language, a tool/application, an API, or a database. Overall, our findings show the promise of using additional context as navigational cues.}
}

@inproceedings{Getseva2022,
  doi = {10.1145/3502718.3524794},
  title = {An empirical analysis of code-tracing concepts},
  booktitle = iticse,
  author = {Getseva, Vanesa and Kumar, Amruth N},
  publisher = {ACM},
  month = jul,
  year = {2022},
  conference = iticse,
  abstract = {Which code-tracing concepts are introductory programming students likely to learn from classroom instruction and which ones need additional problem-solving practice to master? Are there relationships among programming concepts that can be used to build adaptive assessment instruments? To answer these questions, we analyzed the data collected over several semesters by a suite of code-tracing tutors called problets, that administered pre-test, practice, post-test protocol. Each tutor covered a single programming topic, which consisted of 9-25 concepts. For each concept, we used the pretest data to calculate the probability that students knew the concept before using the tutor. Using a weighted average of the concept probabilities, we found that students had learned some topics more than others: if/if-else (0.85), function behavior (0.76), arrays (0.73), while (0.7), for (0.69), switch (0.67), and debugging functions (0.55). Some of the concepts on which students needed additional practice included bugs, nested loops and back-to-back loops. Expressions, even when used in novel contexts, were not challenging for students. We built a Bayesian network for each topic based on conditional probabilities to discover the concepts that must be covered, and those whose coverage is redundant in the presence of other concepts. A strength of this empirical study is that it uses a large dataset collected from multiple institutions over multiple semesters. We also list threats to the validity of the study.}
}

@article{Graziotin2022,
  doi = {10.1145/3469888},
  title = {Psychometrics in behavioral software engineering: A methodological introduction with guidelines},
  author = {Graziotin, Daniel and Lenberg, Per and Feldt, Robert and Wagner, Stefan},
  abstract = {A meaningful and deep understanding of the human aspects of software engineering (SE) requires psychological constructs to be considered. Psychology theory can facilitate the systematic and sound development as well as the adoption of instruments (e.g., psychological tests, questionnaires) to assess these constructs. In particular, to ensure high quality, the psychometric properties of instruments need evaluation. In this article, we provide an introduction to psychometric theory for the evaluation of measurement instruments for SE researchers. We present guidelines that enable using existing instruments and developing new ones adequately. We conducted a comprehensive review of the psychology literature framed by the Standards for Educational and Psychological Testing. We detail activities used when operationalizing new psychological constructs, such as item pooling, item review, pilot testing, item analysis, factor analysis, statistical property of items, reliability, validity, and fairness in testing and test bias. We provide an openly available example of a psychometric evaluation based on our guideline. We hope to encourage a culture change in SE research towards the adoption of established methods from psychology. To improve the quality of behavioral research in SE, studies focusing on introducing, validating, and then using psychometric instruments need to be more common.},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {31},
  number = {1},
  pages = {1--36},
  month = jan,
  year = {2022}
}

@article{Haduong2019,
  doi = {10.1108/ILS-05-2018-0037},
  title = {``I like computers. {I} hate coding'': a portrait of two teens' experiences},
  author = {Haduong, Paulina},
  journal = {Inf. Learn. Sci.},
  publisher = {Emerald},
  volume = {120},
  number = {{5}/6},
  pages = {349--365},
  month = may,
  year = {2019},
  abstract = {Purpose Some empirical evidence suggests that historically marginalized young people may enter introductory programming experiences with skepticism or reluctance, because of negative perceptions of the computing field. This paper aims to explore how learner identity and motivation can affect their experiences in an introductory computer science (CS) experience, particularly for young people who have some prior experience with computing. In this program, learners were asked to develop digital media artifacts about civic issues using Scratch, a block-based programming language.   Design/methodology/approach Through participant observation as a teacher and designer of the course, artifact analysis of student-generated computer programs and design journals, as well as with two follow-up 1-h interviews, the author used the qualitative method of portraiture to examine how two reluctant learners experienced a six-week introductory CS program.   Findings These learners’ experiences illuminate the ways in which identity, community and competence can play a role in supporting learner motivation in CS education experiences.   Research limitations/implications As more students have multiple introductory computing encounters, educators need to take into account not only their perceptions of the computing field more broadly but also specific prior encounters with programming. Because of the chosen research approach, the research results may lack generalizability. Researchers are encouraged to explore other contexts and examples further.   Practical implications This portrait highlights the need for researchers and educators to take into account student motivation in the design of learning environments.   Originality/value This portrait offers a novel examination of novice programmer experiences through the choice in method, as well as new examples of how learner identity can affect student motivation.}
}

@inproceedings{Hartel2022,
  doi = {10.1145/3524842.3527960},
  title = {Operationalizing threats to {MSR} studies by simulation-based testing},
  booktitle = msr,
  author = {H{\"a}rtel, Johannes and L{\"a}mmel, Ralf},
  publisher = {ACM},
  month = may,
  year = {2022},
  abstract = {Quantitative studies on the border between Mining Software Repository (MSR) and Empirical Software Engineering (ESE) apply data analysis methods, like regression modeling, statistic tests or correlation analysis, to commits or pulls to better understand the software development process. Such studies assure the validity of the reported results by following a sound methodology. However, with increasing complexity, parts of the methodology can still go wrong. This may result in MSR/ESE studies with undetected threats to validity. In this paper, we propose to systematically protect against threats by operationalizing their treatment using simulations. A simulation substitutes observed and unobserved data, related to an MSR/ESE scenario, with synthetic data, carefully defined according to plausible assumptions on the scenario. Within a simulation, unobserved data becomes transparent, which is the key difference to a real study, necessary to detect threats to an analysis methodology. Running an analysis methodology on synthetic data may detect basic technical bugs and misinterpretations, but it also improves the trust in the methodology. The contribution of a simulation is to operationalize testing the impact of important assumptions. Assumptions still need to be rated for plausibility. We evaluate simulation-based testing by operationalizing undetected threats in the context of four published MSR/ESE studies. We recommend that future research uses such more systematic treatment of threats, as a contribution against the reproducibility crisis.}
}

@inproceedings{Head2020,
  doi = {10.1145/3313831.3376798},
  title = {Composing flexibly-organized step-by-step tutorials from linked source code, snippets, and outputs},
  booktitle = {Proceedings of the 2020 {CHI} Conference on Human Factors in Computing Systems},
  author = {Head, Andrew and Jiang, Jason and Smith, James and Hearst, Marti A and Hartmann, Bj{\"o}rn},
  publisher = {ACM},
  month = apr,
  year = {2020},
  conference = {CHI '20: CHI Conference on Human Factors in Computing Systems},
  abstract = {Programming tutorials are a pervasive, versatile medium for teaching programming. In this paper, we report on the content and structure of programming tutorials, the pain points authors experience in writing them, and a design for a tool to help improve this process. An interview study with 12 experienced tutorial authors found that they construct documents by interleaving code snippets with text and illustrative outputs. It also revealed that authors must often keep related artifacts of source programs, snippets, and outputs consistent as a program evolves. A content analysis of 200 frequently-referenced tutorials on the web also found that most tutorials contain related artifactsduplicate code and outputs generated from snippetsthat an author would need to keep consistent with each other. To address these needs, we designed a tool called Torii with novel authoring capabilities. An in-lab study showed that tutorial authors can successfully use the tool for the unique affordances identified, and provides guidance for designing future tools for tutorial authoring.}
}

@inproceedings{Hellman2022,
  doi = {10.1145/3528579.3529178},
  title = {Characterizing user behaviors in open-source software user forums},
  booktitle = {Proceedings of the 15th International Conference on Cooperative and Human Aspects of Software Engineering},
  author = {Hellman, Jazlyn and Chen, Jiahao and Uddin, Md Sami and Cheng, Jinghui and Guo, Jin L C},
  publisher = {ACM},
  month = may,
  year = {2022},
  conference = icse,
  abstract = {User forums of Open Source Software (OSS) enable end-users to collaboratively discuss problems concerning the OSS applications. Despite decades of research on OSS, we know very little about how end-users engage with OSS communities on these forums, in particular, the challenges that hinder their continuous and meaningful participation in the OSS community. Many previous works are developer-centric and overlook the importance of end-user forums. As a result, end-users' expectations are seldom reflected in OSS development. To better understand user behaviors in OSS user forums, we carried out an empirical study analyzing about 1.3 million posts from user forums of four popular OSS applications: Zotero, Audacity, VLC, and RStudio. Through analyzing the contribution patterns of three common user types (end-users, developers, and organizers), we observed that end-users not only initiated most of the threads (above 96\% of threads in three projects, 86\% in the other), but also acted as the significant contributors for responding to other users' posts, even though they tended to lack confidence in their activities as indicated by psycho-linguistic analyses. Moreover, we found end-users more open, reflecting a more positive emotion in communication than organizers and developers in the forums. Our work contributes new knowledge about end-users' activities and behaviors in OSS user forums that the vital OSS stakeholders can leverage to improve end-user engagement in the OSS development process.}
}

@article{Hundhausen2022,
  doi = {10.1080/08993408.2022.2071543},
  title = {Assessing individual contributions to software engineering projects: a replication study},
  author = {Hundhausen, C D and Conrad, P T and Carter, A S and Adesope, O},
  journal = {Comput. Sci. Educ.},
  publisher = {Informa UK Limited},
  volume = {32},
  number = {3},
  pages = {335--354},
  month = jul,
  year = {2022},
  abstract = {ABSTRACT Background and Context Assessing team members’ indivdiual contributions to software development projects poses a key problem for computing instructors. While instructors typically rely on subjective assessments, objective assessments could provide a more robust picture. To explore this possibility, In a 2020 paper, Buffardi presented a correlational analysis of objective metrics and subjective metrics in an advanced software engineering project course (n= 41 students and 10 teams), finding only two significant correlations. Objective To explore the robustness of Buffardi’s findings and gain further insight, we conducted a larger scale replication of the Buffardi study (n = 118 students and 25 teams) in three courses at three institutions. Method We collected the same data as in the Buffardi study and computed the same measures from those data. We replicated Buffardi’s exploratory, correlational and regression analyses of objective and subjective measures. Findings While replicating four of Buffardi’s five significant correlational findings and partially replicating the findings of Buffardi’s regression analyses, our results go beyond those of Buffardi by identifying eight additional significant correlations. Implications In contrast to Buffardi’s study, our larger scale study suggests that subjective and objective measures of individual performance in team software development projects can be fruitfully combined to provide consistent and complementary assessments of individual performance.}
}

@article{Huszar2022,
  doi = {10.1073/pnas.2025334119},
  title = {Algorithmic amplification of politics on Twitter},
  author = {Husz{\'a}r, Ferenc and Ktena, Sofia Ira and O'Brien, Conor and Belli, Luca and Schlaikjer, Andrew and Hardt, Moritz},
  abstract = {Content on Twitter's home timeline is selected and ordered by personalization algorithms. By consistently ranking certain content higher, these algorithms may amplify some messages while reducing the visibility of others. There's been intense public and scholarly debate about the possibility that some political groups benefit more from algorithmic amplification than others. We provide quantitative evidence from a long-running, massive-scale randomized experiment on the Twitter platform that committed a randomized control group including nearly 2 million daily active accounts to a reverse-chronological content feed free of algorithmic personalization. We present two sets of findings. First, we studied tweets by elected legislators from major political parties in seven countries. Our results reveal a remarkably consistent trend: In six out of seven countries studied, the mainstream political right enjoys higher algorithmic amplification than the mainstream political left. Consistent with this overall trend, our second set of findings studying the US media landscape revealed that algorithmic amplification favors right-leaning news sources. We further looked at whether algorithms amplify far-left and far-right political groups more than moderate ones; contrary to prevailing public belief, we did not find evidence to support this hypothesis. We hope our findings will contribute to an evidence-based debate on the role personalization algorithms play in shaping political content consumption.},
  journal = {Proc. Natl. Acad. Sci. U. S. A.},
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {1},
  pages = {e2025334119},
  month = jan,
  year = {2022},
  keywords = {algorithmic personalization; media amplification; political bias; social media},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  abstract = {Significance The role of social media in political discourse has been the topic of intense scholarly and public debate. Politicians and commentators from all sides allege that Twitter’s algorithms amplify their opponents’ voices, or silence theirs. Policy makers and researchers have thus called for increased transparency on how algorithms influence exposure to political content on the platform. Based on a massive-scale experiment involving millions of Twitter users, a fine-grained analysis of political parties in seven countries, and 6.2 million news articles shared in the United States, this study carries out the most comprehensive audit of an algorithmic recommender system and its effects on political content. Results unveil that the political right enjoys higher amplification compared to the political left. Content on Twitter’s home timeline is selected and ordered by personalization algorithms. By consistently ranking certain content higher, these algorithms may amplify some messages while reducing the visibility of others. There’s been intense public and scholarly debate about the possibility that some political groups benefit more from algorithmic amplification than others. We provide quantitative evidence from a long-running, massive-scale randomized experiment on the Twitter platform that committed a randomized control group including nearly 2 million daily active accounts to a reverse-chronological content feed free of algorithmic personalization. We present two sets of findings. First, we studied tweets by elected legislators from major political parties in seven countries. Our results reveal a remarkably consistent trend: In six out of seven countries studied, the mainstream political right enjoys higher algorithmic amplification than the mainstream political left. Consistent with this overall trend, our second set of findings studying the US media landscape revealed that algorithmic amplification favors right-leaning news sources. We further looked at whether algorithms amplify far-left and far-right political groups more than moderate ones; contrary to prevailing public belief, we did not find evidence to support this hypothesis. We hope our findings will contribute to an evidence-based debate on the role personalization algorithms play in shaping political content consumption.}
}

@article{Idowu2022,
  doi = {10.1145/3543847},
  title = {Asset management in machine learning: State-of-research and state-of-practice},
  author = {Idowu, Samuel and Str{\"u}ber, Daniel and Berger, Thorsten},
  journal = {ACM Comput. Surv.},
  publisher = {Association for Computing Machinery (ACM)},
  month = jun,
  year = {2022},
  abstract = {Machine learning components are essential for today’s software systems, causing a need to adapt traditional software engineering practices when developing machine-learning-based systems. This need is pronounced due to many development-related challenges of machine learning components such as asset, experiment, and dependency management. Recently, many asset management tools addressing these challenges have become available. It is essential to understand the support such tools offer to facilitate research and practice on building new management tools with native supports for machine learning and software engineering assets. This article positions machine learning asset management as a discipline that provides improved methods and tools for performing operations on machine learning assets. We present a feature-based survey of 18 state-of-practice and 12 state-of-research tools supporting machine-learning asset management. We overview their features for managing the types of assets used in machine learning experiments. Most state-of-research tools focus on tracking, exploring, and retrieving assets to address development concerns such as reproducibility, while the state-of-practice tools also offer collaboration and workflow-execution-related operations. In addition, assets are primarily tracked intrusively from the source code through APIs and managed via web dashboards or command-line interfaces. We identify asynchronous collaboration and asset reusability as directions for new tools and techniques.}
}

@article{CanovasIzquierdo2022,
  doi = {10.1007/s10664-021-10061-x},
  title = {On the analysis of non-coding roles in open source development},
  author = {C{\'a}novas Izquierdo, Javier Luis and Cabot, Jordi},
  abstract = {AbstractThe role of non-coding contributors in Open Source Software (OSS) is poorly understood. Most of current research around OSS development focuses on the coding aspects of the project (e.g., commits, pull requests or code reviews) while ignoring the potential of other types of contributions. Often, due to the assumption that these other contributions are not significant in number and that, in any case, they are handled by the same people that are also part of the ``coding team''. This paper aims to investigate whether this is actually the case by analyzing the frequency and diversity of non-coding contributions in OSS development. As a sample of projects for our study we have taken the 100 most popular projects in the ecosystem of NPM, a package manager for JavaScript. Our results validate the importance of dedicated non-coding contributors in OSS and the diversity of OSS communities as, typically, a contributor specializes in a specific subset of roles. We foresee that projects adopting explicit policies to attract and onboard them could see a positive impact in their long-term sustainability providing they also put in place the right governance strategies to facilitate the migration and collaboration among the different roles. As part of this work, we also provide a replicability package to facilitate further quantitative role-based analysis by other researchers.},
  journal = {Empir. Softw. Eng.},
  publisher = {Springer Science and Business Media LLC},
  volume = {27},
  number = {1},
  month = jan,
  year = {2022},
  copyright = {https://creativecommons.org/licenses/by/4.0}
}

@inproceedings{Jeffries2022,
  doi = {10.1145/3502718.3524809},
  title = {115 ways not to say hello, world!},
  booktitle = iticse,
  author = {Jeffries, Bryn and Lee, Jung A and Koprinska, Irena},
  publisher = {ACM},
  month = jul,
  year = {2022},
  conference = iticse,
  abstract = {Online programming courses can provide detailed automatic feedback for code that fails to meet various test conditions, but novice students often struggle with syntax errors and are unable to write valid testable code. Even for very simple exercises, the range of incorrect code can be surprising to educators with mastery of a programming language. This research paper presents an analysis of the error messages from code run by students in an introductory Python~3 programming course, participated in by 8680 primary and high-school students from 680 institutions. The invalid programs demonstrate a wide diversity of mistakes: even for a one-line \"Hello World!'' exercise there were 115 unique invalid programs. The most common errors are identified and compared to the topics introduced in the course. The most generic errors in selected exercises are investigated in greater detail to understand the underlying causes. While the majority of students attempting an exercise reach a successful outcome, many students encounter at least one error in their code. Of these, many such errors indicate basic mistakes, such as unquoted string literals, even in exercises late in the course for which some proficiency of earlier concepts is assumed. These observations suggest there is significant scope to provide greater reinforcement of students' understanding of earlier concepts.}
}

@article{Joblin2022,
  doi = {10.1145/3504003},
  title = {How do successful and failed projects differ? A Socio-technical analysis},
  author = {Joblin, Mitchell and Apel, Sven},
  abstract = {Software development is at the intersection of the social realm , involving people who develop the software, and the technical realm , involving artifacts (code, docs, etc.) that are being produced. It has been shown that a socio-technical perspective provides rich information about the state of a software project. In particular, we are interested in socio-technical factors that are associated with project success . For this purpose, we frame the task as a network classification problem. We show how a set of heterogeneous networks composed of social and technical entities can be jointly embedded in a single vector space enabling mathematically sound comparisons between distinct software projects. Our approach is specifically designed using intuitive metrics stemming from network analysis and statistics to ease the interpretation of results in the context of software engineering wisdom. Based on a selection of 32 open-source projects, we perform an empirical study to validate our approach considering three prediction scenarios to test the classification model's ability generalizing to: (1) randomly held-out project snapshots, (2) future project states, and (3) entirely new projects. Our results provide evidence that a socio-technical perspective is superior to a pure social or technical perspective when it comes to early indicators of future project success. To our surprise, the methodology proposed here even shows evidence of being able to generalize to entirely novel (project hold-out set) software projects reaching predication accuracies of 80\%, which is a further testament to the efficacy of our approach and beyond what has been possible so far. In addition, we identify key features that are strongly associated with project success. Our results indicate that even relatively simple socio-technical networks capture highly relevant and interpretable information about the early indicators of future project success.},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  publisher = {Association for Computing Machinery (ACM)},
  month = feb,
  year = {2022},
  abstract = {Software development is at the intersection of the social realm, involving people who develop the software, and the technical realm, involving artifacts (code, docs, etc.) that are being produced. It has been shown that a socio-technical perspective provides rich information about the state of a software project. In particular, we are interested in socio-technical factors that are associated with project success. For this purpose, we frame the task as a network classification problem. We show how a set of heterogeneous networks composed of social and technical entities can be jointly embedded in a single vector space enabling mathematically sound comparisons between distinct software projects. Our approach is specifically designed using intuitive metrics stemming from network analysis and statistics to ease the interpretation of results in the context of software engineering wisdom. Based on a selection of 32 open source projects, we perform an empirical study to validate our approach considering three prediction scenarios to test the classification model’s ability generalizing to (1) randomly held-out project snapshots, (2) future project states, and (3) entirely new projects. Our results provide evidence that a socio-technical perspective is superior to a pure social or technical perspective when it comes to early indicators of future project success. To our surprise, the methodology proposed here even shows evidence of being able to generalize to entirely novel (project hold-out set) software projects reaching predication accuracies of 80%, which is a further testament to the efficacy of our approach and beyond what has been possible so far. In addition, we identify key features that are strongly associated with project success. Our results indicate that even relatively simple socio-technical networks capture highly relevant and interpretable information about the early indicators of future project success.}
}

@inproceedings{Kacsmar2022,
  doi = {10.1145/3564721.3564739},
  title = {Improving interactive instruction: Faculty engagement requires starting small and telling all},
  booktitle = {Koli Calling '22: 22nd Koli Calling International Conference on Computing Education Research},
  author = {Kacsmar, Bailey},
  publisher = {ACM},
  month = nov,
  year = {2022},
  conference = {Koli 2022: 22nd Koli Calling International Conference on Computing Education Research},
  abstract = {Interactive instruction, such as student-centered learning or active learning, is known to benefit student success as well as diversity in computer science. However, there is a persistent and substantial dissonance between research and practice of computer science education techniques. Current research on computer science education, while extensive, sees limited adoption beyond the original researchers. The developed educational technologies can lack sufficient detail for replication or be too specific and require extensive reworking to be employable by other instructors. Furthermore, instructors face barriers to adopting interactive techniques within their classroom due to student reception, resources, and awareness. We argue that the advancement of computer science education, in terms of propagation and sustainability of student-centered teaching, requires guided approaches for incremental instructional changes as opposed to revolutionary pedagogy. This requires the prioritization of lightweight techniques that can fit within existing lecture formats to enable instructors to overcome barriers hindering the adoption of interactive techniques. Furthermore, such techniques and innovations must be documented in the form of computing education research artifacts, building upon the practices of software artifacts.}
}

@article{Kohavi2009,
  doi = {10.1007/s10618-008-0114-1},
  title = {Controlled experiments on the web: survey and practical guide},
  author = {Kohavi, Ron and Longbotham, Roger and Sommerfield, Dan and Henne, Randal M},
  journal = {Data Min. Knowl. Discov.},
  publisher = {Springer Science and Business Media LLC},
  volume = {18},
  number = {1},
  pages = {140--181},
  month = feb,
  year = {2009},
  copyright = {https://creativecommons.org/licenses/by-nc/2.0},
  abstract = {}
}

@inproceedings{Kumar2022,
  doi = {10.1145/3524610.3528389},
  title = {On the developers' attitude towards {CRAN} checks},
  booktitle = icpc,
  author = {Kumar, Pranjay and Ie, Davin and Vidoni, Melina},
  publisher = {ACM},
  month = may,
  year = {2022},
  conference = icpc,
  abstract = {R is a package-based, multi-paradigm programming language for scientific software. It provides an easy way to install third-party code, datasets, tests, documentation and examples through CRAN (Comprehensive R Archive Network). Prior works indicated developers tend to code workarounds to bypass CRAN's automated checks (performed when submitting a package) instead of fixing the code-doing so reduces packages' quality. It may become a threat to those analyses written in R that rely on miss-checked code. This preliminary study card-sorted source code comments and analysed StackOverflow (SO) conversations discussing CRAN checks to understand developers' attitudes. We determined that about a quarter of SO posts aim to bypass a check with a workaround; the most affected are code-related problems, package dependencies, installation and feasibility. We analyse these checks and outline future steps to improve similar automated analyses.}
}

@article{Lawrence-Dill2022,
  doi = {10.1371/journal.pcbi.1009957},
  title = {Ten simple rules to ruin a collaborative environment},
  author = {Lawrence-Dill, Carolyn J and Allscheid, Robyn L and Boaitey, Albert and Bauman, Todd and Buckler, 4th, Edward S and Clarke, Jennifer L and Cullis, Christopher and Dekkers, Jack and Dorius, Cassandra J and Dorius, Shawn F and Ertl, David and Homann, Matthew and Hu, Guiping and Losch, Mary and Lyons, Eric and Murdoch, Brenda and Navabi, Zahra-Katy and Punnuri, Somashekhar and Rafiq, Fahad and Reecy, James M and Schnable, Patrick S and Scott, Nicole M and Sheehan, Moira and Sirault, Xavier and Staton, Margaret and Tuggle, Christopher K and Van Eenennaam, Alison and Voas, Rachael},
  journal = {PLoS Comput. Biol.},
  publisher = {Public Library of Science (PLoS)},
  volume = {18},
  number = {4},
  pages = {e1009957},
  month = apr,
  year = {2022},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  abstract = {}
}

@inproceedings{Li2019,
  doi = {10.1145/3286960.3286970},
  title = {Towards a framework for teaching debugging},
  booktitle = {Proceedings of the {Twenty-First} Australasian Computing Education Conference on - {ACE} '19},
  author = {Li, Chen and Chan, Emily and Denny, Paul and Luxton-Reilly, Andrew and Tempero, Ewan},
  abstract = {Debugging is an important component of software development, yet most novice programmers are not explicitly taught to apply systematic strategies or processes for debugging. In this paper we adapt a framework developed for teaching troubleshooting to the debugging domain, and explore how the literature on teaching debugging maps to this framework. We identify debugging processes that are fundamental for novices to learn, aspects of debugging that novices typically struggle to develop, and shortcomings of tools designed to support teaching of debugging.},
  publisher = {ACM Press},
  year = {2019},
  copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  conference = {the Twenty-First Australasian Computing Education Conference},
  abstract = {Debugging is an important component of software development, yet most novice programmers are not explicitly taught to apply systematic strategies or processes for debugging. In this paper we adapt a framework developed for teaching troubleshooting to the debugging domain, and explore how the literature on teaching debugging maps to this framework. We identify debugging processes that are fundamental for novices to learn, aspects of debugging that novices typically struggle to develop, and shortcomings of tools designed to support teaching of debugging.}
}

@inproceedings{Liang2022,
  doi = {10.1145/3540250.3549082},
  title = {Understanding skills for {OSS} communities on {GitHub}},
  booktitle = esec-fse,
  author = {Liang, Jenny T and Zimmermann, Thomas and Ford, Denae},
  publisher = {ACM},
  month = nov,
  year = {2022},
  conference = esec-fse,
  abstract = {The development of open source software (OSS) is a broad field which requires diverse skill sets. For example, maintainers help lead the project and promote its longevity, technical writers assist with documentation, bug reporters identify defects in software, and developers program the software. However, it is unknown which skills are used in OSS development as well as OSS contributors' general attitudes towards skills in OSS. In this paper, we address this gap by administering a survey to a diverse set of 455 OSS contributors. Guided by these responses as well as prior literature on software development expertise and social factors of OSS, we develop a model of skills in OSS that considers the many contexts OSS contributors work in. This model has 45 skills in the following 9 categories: technical skills, working styles, problem solving, contribution types, project-specific skills, interpersonal skills, external relations, management, and characteristics. Through a mix of qualitative and quantitative analyses, we find that OSS contributors are actively motivated to improve skills and perceive many benefits in sharing their skills with others. We then use this analysis to derive a set of design implications and best practices for those who incorporate skills into OSS tools and platforms, such as GitHub.}
}

@article{Lu2021,
  doi = {10.22152/programming-journal.org/2022/6/8},
  title = {Types for tables: A language design benchmark},
  author = {Lu, Kuang-Chen and Greenman, Ben and Krishnamurthi, Shriram},
  journal = {Art Sci. Eng. Program.},
  publisher = {Aspect-Oriented Software Association (AOSA)},
  volume = {6},
  number = {2},
  month = nov,
  year = {2021},
  abstract = {Context Tables are ubiquitous formats for data. Therefore, techniques for writing correct programs over tables, and debugging incorrect ones, are vital. Our speciﬁc focus in this paper is on rich types that articulate the properties of tabular operations. We wish to study both their expressive power and diagnostic quality . Inquiry There is no “standard library” of table operations. As a result, every paper (and project) is free to use its own (sub)set of operations. This makes artifacts very diﬃcult to compare, and it can be hard to tell whether omitted operations were left out by oversight or because they cannot actually be expressed. Furthermore, virtually no papers discuss the quality of type error feedback. Approach We combed through several existing languages and libraries to create a “standard library” of table operations. Each entry is accompanied by a detailed speciﬁcation of its “type,” expressed independent of (and hence not constrained by) any type language. We also studied and categorized a corpus of (student) program edits that resulted in table-related errors. We used this to generate a suite of erroneous programs. Finally, we adapted the concept of a datasheet to facilitate comparisons of diﬀerent implementations. Knowledge Our benchmark creates a common ground to frame work in this area. Language designers who claim to support typed programming over tables have a clear suite against which to demonstrate their system’s expressive power. Our family of errors also gives them a chance to demonstrate the quality of feedback. Researchers who improve one aspect—especially error reporting—without changing the other can demonstrate their improvement, as can those who engage in trade-oﬀs between the two. The net result should be much better science in both expressiveness and diagnostics. We also introduce a datasheet format for presenting this knowledge in a methodical way. ubiquitous, and the expressive power of type systems keeps growing. Our benchmark and datasheet can help lead to more orderly science. It also beneﬁts programmers trying to choose a language.}
}

@inproceedings{Luders2022,
  doi = {10.1145/3524842.3528457},
  title = {Beyond duplicates},
  booktitle = msr,
  author = {L{\"u}ders, Clara Marie and Bouraffa, Abir and Maalej, Walid},
  publisher = {ACM},
  month = may,
  year = {2022},
  abstract = {Software projects use Issue Tracking Systems (ITS) like JIRA to track issues and organize the workflows around them. Issues are often inter-connected via different links such as the default JIRA link types Duplicate, Relate, Block, or Subtask. While previous research has mostly focused on analyzing and predicting duplication links, this work aims at understanding the various other link types, their prevalence, and characteristics towards a more reliable link type prediction. For this, we studied 607,208 links connecting 698,790 issues in 15 public JIRA repositories. Besides the default types, the custom types Depend, Incorporate, Split, and Cause were also common. We manually grouped all 75 link types used in the repositories into five general categories: General Relation, Duplication, Composition, Temporal/Causal, and Workflow. Comparing the structures of the corresponding graphs, we observed several trends. For instance, Duplication links tend to represent simpler issue graphs often with two components and Composition links present the highest amount of hierarchical tree structures (97.7%). Surprisingly, General Relation links have a significantly higher transitivity score than Duplication and Temporal/ Causal links. Motivated by the differences between the link types and by their popularity, we evaluated the robustness of two state-of-the-art duplicate detection approaches from the literature on the JIRA dataset. We found that current deep-learning approaches confuse between Duplication and other links in almost all repositories. On average, the classification accuracy dropped by 6% for one approach and 12% for the other. Extending the training sets with other link types seems to partly solve this issue. We discuss our findings and their implications for research and practice.}
}

@article{Martin2023,
  doi = {10.1016/j.iheduc.2022.100879},
  title = {Bichronous online learning: Award-winning online instructor practices of blending asynchronous and synchronous online modalities},
  author = {Martin, Florence and Kumar, Swapna and Ritzhaupt, Albert D and Polly, Drew},
  journal = {Internet High. Educ.},
  publisher = {Elsevier BV},
  volume = {56},
  number = {100879},
  pages = {100879},
  month = jan,
  year = {2023},
  abstract = {}
}

@inproceedings{Miedema2022,
  doi = {10.1145/3524610.3529158},
  title = {So many brackets!},
  booktitle = icpc,
  author = {Miedema, Daphne and Fletcher, George and Aivaloglou, Efthimia},
  publisher = {ACM},
  month = may,
  year = {2022},
  conference = icpc,
  abstract = {The Structured Query Language (SQL) is a widely taught database query language in computer science, data science, and software engineering programs. While highly expressive, SQL is challenging to learn for novices. Various research has explored the errors and mistakes that SQL users make. Specific attributes of SQL code, such as the number of tables and the degree of nesting, have been found to impact its understandability and maintainability. Furthermore, prior studies have shown that novices have significant issues using SQL correctly, due to factors such as expressive ease, existing knowledge and misconceptions, and the impact of cognitive load. In this paper we identify another factor: self-inflicted query complexity, where users hinder their own problem solving process. We analyse 8K intermediate and final student attempts to six SQL exer-cises, approaching complexity from four perspective: correctness, execution order, edit distance and query intricacy. Through our analyses, we find that our students are hindered in their query formulation process by mismanaging complexity through writing overly elaborate queries containing unnecessary elements, overusing brackets and nesting, and incrementally building queries with persistent errors.}
}

@inproceedings{Nguyen2022,
  doi = {10.1145/3524842.3528470},
  title = {An empirical evaluation of {GitHub} copilot's code suggestions},
  booktitle = msr,
  author = {Nguyen, Nhan and Nadi, Sarah},
  publisher = {ACM},
  month = may,
  year = {2022},
  abstract = {GitHub and OpenAI recently launched Copilot, an “AI pair programmer” that utilizes the power of Natural Language Processing, Static Analysis, Code Synthesis, and Artificial Intelligence. Given a natural language description of the target functionality, Copilot can generate corresponding code in several programming languages. In this paper, we perform an empirical study to evaluate the correctness and understandability of Copilot's suggested code. We use 33 LeetCode questions to create queries for Copilot in four different programming languages. We evaluate the correctness of the corresponding 132 Copilot solutions by running LeetCode's provided tests, and evaluate understandability using SonarQube's cyclomatic complexity and cognitive complexity metrics. We find that Copilot's Java suggestions have the highest correctness score (57%) while JavaScript is the lowest (27%). Overall, Copilot's suggestions have low complexity with no notable differences between the programming languages. We also find some potential Copilot shortcomings, such as generating code that can be further simplified and code that relies on undefined helper methods.}
}

@inproceedings{Pereira2017,
  doi = {10.1145/3136014.3136031},
  title = {Energy efficiency across programming languages: how do energy, time, and memory relate?},
  booktitle = {Proceedings of the 10th {ACM} {SIGPLAN} International Conference on Software Language Engineering},
  author = {Pereira, Rui and Couto, Marco and Ribeiro, Francisco and Rua, Rui and Cunha, J{\'a}come and Fernandes, Jo{\~a}o Paulo and Saraiva, Jo{\~a}o},
  abstract = {This paper presents a study of the runtime, memory usage and energy consumption of twenty seven well-known software languages. We monitor the performance of such languages using ten different programming problems, expressed in each of the languages. Our results show interesting findings, such as, slower/faster languages consuming less/more energy, and how memory usage influences energy consumption. Finally, we show how to use our results to provide software engineers support to decide which language to use when energy efficiency is a concern.},
  publisher = {ACM},
  month = oct,
  year = {2017},
  copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  conference = splash,
  abstract = {This paper presents a study of the runtime, memory usage and energy consumption of twenty seven well-known software languages. We monitor the performance of such languages using ten different programming problems, expressed in each of the languages. Our results show interesting findings, such as, slower/faster languages consuming less/more energy, and how memory usage influences energy consumption. Finally, we show how to use our results to provide software engineers support to decide which language to use when energy efficiency is a concern.}
}

@article{Prana2019,
  doi = {10.1007/s10664-018-9660-3},
  title = {Categorizing the content of {GitHub} {README} files},
  author = {Prana, Gede Artha Azriadi and Treude, Christoph and Thung, Ferdian and Atapattu, Thushari and Lo, David},
  journal = {Empir. Softw. Eng.},
  publisher = {Springer Science and Business Media LLC},
  volume = {24},
  number = {3},
  pages = {1296--1327},
  month = jun,
  year = {2019},
  abstract = {}
}

@inproceedings{Presler-Marshall2022,
  doi = {10.1145/3478431.3499367},
  title = {Identifying struggling teams in software engineering courses through weekly surveys},
  booktitle = sigcse,
  author = {Presler-Marshall, Kai and Heckman, Sarah and Stolee, Kathryn T},
  publisher = {ACM},
  month = feb,
  year = {2022},
  copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  conference = sigcse,
  abstract = {Teaming is increasingly a core aspect of professional software engineering and most undergraduate computer science curricula. At NC State University, we teach communication and project-management skills explicitly through a junior-level software engineering course. However, some students may have a dysfunctional team experience that imperils their ability to learn these skills. Identifying these teams during a team project is important so the teaching staff can intervene early and hopefully alleviate the issues. We propose a weekly reflection survey to help the course teaching staff proactively identify teams that may not be on track to learn the course outcomes. The questions on the survey focus on team communication and collaboration over the previous week. We evaluate our survey on two semesters of the undergraduate software engineering course by comparing teams with poor end-of-project grades or peer evaluations against teams flagged on a weekly basis through the surveys. We find that the survey can identify most teams that later struggled on the project, typically by the half-way mark of the project, and thus may provide instructors with an actionable early-warning about struggling teams. Furthermore, a majority of students (64.4%) found the survey to be a helpful tool for keeping their team on track. Finally, we discuss future work for improving the survey and engaging with student teams.}
}

@inproceedings{Presler-Marshall2022,
  doi = {10.1145/3501385.3543980},
  title = {What makes team[s] work? A study of team characteristics in software engineering projects},
  booktitle = icer,
  author = {Presler-Marshall, Kai and Heckman, Sarah and Stolee, Kathryn T},
  publisher = {ACM},
  month = aug,
  year = {2022},
  conference = icer,
  abstract = {Teaming is a core component in practically all professional software engineering careers, and as such, is a key skill taught in many undergraduate Computer Science programs. However, not all teams manage to work together effectively, and in education, this can deprive some students of successful teaming experiences. In this work, we seek to gain insights into the characteristics of successful and unsuccessful undergraduate student teams in a software engineering course. We conduct semi-structured interviews with 18 students who have recently completed a team-based software engineering course to understand how they worked together, what challenges they faced, and how they tried to overcome these challenges. Our results show that common problems include communicating, setting and holding to deadlines, and effectively identifying tasks and their relative difficulty. Additionally, we find that self-reflection on what is working and not working or external motivators such as grades help some, but not all, teams overcome these challenges. Finally, we conclude with recommendations for educators on successful behaviours to steer teams towards, and recommendations for researchers on future work to better understand challenges that teams face.}
}

@article{Qamar2022,
  doi = {10.1016/j.infsof.2022.106972},
  title = {Taxonomy of bug tracking process smells: Perceptions of practitioners and an empirical analysis},
  author = {Qamar, Khushbakht Ali and S{\"u}l{\"u}n, Emre and T{\"u}z{\"u}n, Eray},
  journal = {Inf. Softw. Technol.},
  publisher = {Elsevier BV},
  volume = {150},
  number = {106972},
  pages = {106972},
  month = oct,
  year = {2022},
  abstract = {}
}

@article{Queiroz2022,
  doi = {10.1080/21650349.2022.2088623},
  title = {Science as a game: conceptual model and application in scientific software design},
  author = {Queiroz, Francisco and Lonsdale, Maria and Spitz, Rejane},
  journal = {Int. j. des. creat. innov.},
  publisher = {Informa UK Limited},
  volume = {10},
  number = {4},
  pages = {222--246},
  month = oct,
  year = {2022},
  abstract = {ABSTRACT Scientific inquiry is often described as, and compared to, a game. This paper expands on that analogy to propose a conceptual model of scientific practice built upon Jesper Juul’s game definition, and informed by parallels between the two activities collected from selected works from history and philosophy of science. Moreover, the paper presents a design method, based on the model described, for fostering creative solutions in scientific software user interface design. Results from pilot case studies suggest both model and method are helpful, allowing participants to describe requirements and ideate solutions, as well providing a framework for the exploration of the game-science analogy within the context of scientific research conducted through computational resources.}
}

@article{Rule2019,
  doi = {10.1371/journal.pcbi.1007007},
  title = {Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks},
  author = {Rule, Adam and Birmingham, Amanda and Zuniga, Cristal and Altintas, Ilkay and Huang, Shih-Cheng and Knight, Rob and Moshiri, Niema and Nguyen, Mai H and Rosenthal, Sara Brin and P{\'e}rez, Fernando and Rose, Peter W},
  journal = {PLoS Comput. Biol.},
  publisher = {Public Library of Science (PLoS)},
  volume = {15},
  number = {7},
  pages = {e1007007},
  month = jul,
  year = {2019},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  abstract = {1 Design Lab, UC San Diego, La Jolla, California, United States of America, 2 Center for Computational Biology and Bioinformatics, UC San Diego, La Jolla, California, United States of America, 3 Department of Pediatrics, UC San Diego, La Jolla, California, United States of America, 4 Data Science Hub, San Diego Supercomputer Center, UC San Diego, La Jolla, California, United States of America, 5 Departments of Bioengineering, and Computer Science and Engineering, and Center for Microbiome Innovation, UC San Diego, La Jolla, California, United States of America, 6 Bioinformatics and Systems Biology Graduate Program, UC San Diego, La Jolla, California, United States of America, 7 Department of Statistics and Berkeley Institute for Data Science, UC Berkeley, and Lawrence Berkeley National Laboratory, Berkeley, California, United States of America}
}

@inproceedings{Sanders2019,
  doi = {10.1145/3291279.3339408},
  title = {Inferential statistics in computing education research},
  booktitle = icer,
  author = {Sanders, Kate and Sheard, Judy and Becker, Brett A and Eckerdal, Anna and Hamouda, Sally and {Simon}},
  publisher = {ACM},
  month = jul,
  year = {2019},
  copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  conference = icer,
  abstract = {The goal of most computing education research is to effect positive change in how computing is taught and learned. Statistical techniques are one important tool for achieving this goal. In this paper we report on an analysis of ICER papers that use inferential statistics. We present the most commonly used techniques; an overview of the techniques the ICER community has used over its first 14 years of papers, grouped according to the purpose of the technique; and a detailed analysis of three of the most commonly used techniques (t-test, chi-squared test, and Mann-Whitney-Wilcoxon). We identify common flaws in reporting and give examples of papers where statistics are reported well. In sum, the paper draws a picture of the use of inferential statistics by the ICER community. This picture is intended to help orient researchers who are new to the use of statistics in computing education research and to encourage reflection by the ICER community on how it uses statistics and how it can improve that use.}
}

@article{Nand_Sharma2022,
  doi = {10.1002/spe.3128},
  title = {Unearthing open source decision‐making processes: A case study of python enhancement proposals},
  author = {Nand Sharma, Pankajeshwara and Tony Roy Savarimuthu, Bastin and Stanger, Nigel},
  journal = {Softw. Pract. Exp.},
  publisher = {Wiley},
  volume = {52},
  number = {10},
  pages = {2312--2346},
  month = oct,
  year = {2022},
  copyright = {http://creativecommons.org/licenses/by-nc/4.0/},
  abstract = {Good governance practices are pivotal to the success of Open Source Software (OSS) projects. However, the decision‐making processes that are made available to stakeholders are at times incomplete and may remain buried and hidden in large amounts of software repository data. This work bridges this gap by unearthing enacted decision‐making processes available for Python Enhancement Proposals (PEPs) from 1.54 million email messages that embody decisions made during the evolution of the Python language. This work employs a design science approach in operationalizing a framework called DeMaP miner that is used to discover hidden processes using information retrieval and information extraction techniques. It also uses process mining techniques to visualize the processes, and comparative structural analysis techniques to compare different decision processes. The work identifies a richer set of decision‐making activities than those reported on the Python website and in prior research work (48 new decision activities, 199 new pathways and 6 new stages). The extracted decision process has been positively evaluated by a prominent member of the Python steering council. The extracted process can be used for process compliance checking and process improvement in OSS communities. Additionally, the DeMaP Miner framework can be extended and customized to suit other OSS projects, such as the OpenJDK project.}
}

@inproceedings{Shome2022,
  doi = {10.1145/3522664.3528621},
  title = {Data smells in public datasets},
  booktitle = {Proceedings of the 1st International Conference on {AI} Engineering: Software Engineering for {AI}},
  author = {Shome, Arumoy and Cruz, Lu{\'\i}s and van Deursen, Arie},
  publisher = {ACM},
  month = may,
  year = {2022},
  conference = {CAIN '22: 1st Conference on AI Engineering - Software Engineering for AI},
  abstract = {The adoption of Artificial Intelligence (AI) in high-stakes domains such as healthcare, wildlife preservation, autonomous driving and criminal justice system calls for a data-centric approach to AI. Data scientists spend the majority of their time studying and wrangling the data, yet tools to aid them with data analysis are lacking. This study identifies the recurrent data quality issues in public datasets. Analogous to code smells, we introduce a novel catalogue of data smells that can be used to indicate early signs of problems or technical debt in machine learning systems. To understand the prevalence of data quality issues in datasets, we analyse 25 public datasets and identify 14 data smells.}
}

@inproceedings{Shrestha2021,
  doi = {10.1145/3472749.3474744},
  title = {Unravel: A Fluent Code Explorer for Data Wrangling},
  booktitle = {The 34th Annual {ACM} Symposium on User Interface Software and Technology},
  author = {Shrestha, Nischal and Barik, Titus and Parnin, Chris},
  publisher = {ACM},
  month = oct,
  year = {2021},
  conference = {UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology},
  abstract = {Data scientists have adopted a popular design pattern in programming called the fluent interface for composing data wrangling code. The fluent interface works by combining multiple transformations on a data table—or dataframes—with a single chain of expressions, which produces an output. Although fluent code promotes legibility, the intermediate dataframes are lost, forcing data scientists to unravel the chain through tedious code edits and re-execution. Existing tools for data scientists do not allow easy exploration or support understanding of fluent code. To address this gap, we designed a tool called Unravel that enables structural edits via drag-and-drop and toggle switch interactions to help data scientists explore and understand fluent code. Data scientists can apply simple structural edits via drag-and-drop and toggle switch interactions to reorder and (un)comment lines. To help data scientists understand fluent code, Unravel provides function summaries and always-on visualizations highlighting important changes to a dataframe. We discuss the design motivations behind Unravel and how it helps understand and explore fluent code. In a first-use study with 14 data scientists, we found that Unravel facilitated diverse activities such as validating assumptions about the code or data, exploring alternatives, and revealing function behavior.}
}

@inproceedings{Silva2022,
  doi = {10.1145/3502718.3524822},
  title = {{DBSnap-eval}},
  booktitle = iticse,
  author = {Silva, Yasin N and Loza, Alexis and Razente, Humberto},
  publisher = {ACM},
  month = jul,
  year = {2022},
  conference = iticse,
  abstract = {Learning to construct database queries can be a challenging task because students need to learn the specific query language syntax as well as properly understand the effect of each query operator and how multiple operators interact in a query. While some previous studies have looked into the types of database query errors students make and how the availability of expected query results can help to increase the success rate, there is very little that is known regarding the patterns that emerge while students are constructing a query. To be able to look into the process of constructing a query, in this paper we introduce DBSnap-Eval, a tool that supports tree-based queries (similar to SQL query plans) and a block-based querying interface to help separate the syntax and semantics of a query. DBSnap-Eval closely monitors the actions students take to construct a query such as adding a dataset or connecting a dataset with an operator. This paper presents an initial set of results about database query construction patterns using DBSnap-Eval. Particularly, it reports identified patterns in the process students follow to answer common database queries.}
}

@article{Soltani2020,
  doi = {10.1007/s10664-020-09882-z},
  title = {The significance of bug report elements},
  author = {Soltani, Mozhan and Hermans, Felienne and B{\"a}ck, Thomas},
  abstract = {AbstractOpen source software projects often use issue repositories, where project contributors submit bug reports. Using these repositories, more bugs in software projects may be identified and fixed. However, the content and therefore quality of bug reports vary. In this study, we aim to understand the significance of different elements in bug reports. We interviewed 35 developers to gain insights into their perceptions on the importance of various contents in bug reports. To assess our findings, we surveyed 305 developers. The results show developers find it highly important that bug reports include crash description, reproducing steps or test cases, and stack traces. Software version, fix suggestions, code snippets, and attached contents have lower importance for software debugging. Furthermore, to evaluate the quality of currently available bug reports, we mined issue repositories of 250 most popular projects on Github. Statistical analysis on the mined issues shows that crash reproducing steps, stack traces, fix suggestions, and user contents, have statistically significant impact on bug resolution times, for ∼70\%, ∼76\%, ∼55\%, and ∼33\% of the projects. However, on avarage, over 70\% of bug reports lack these elements.},
  journal = {Empir. Softw. Eng.},
  publisher = {Springer Science and Business Media LLC},
  volume = {25},
  number = {6},
  pages = {5255--5294},
  month = nov,
  year = {2020},
  copyright = {https://creativecommons.org/licenses/by/4.0}
}

@article{Spinellis2018,
  doi = {10.1145/3186278},
  title = {Modern debugging},
  author = {Spinellis, Diomidis},
  journal = {Commun. ACM},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {61},
  number = {11},
  pages = {124--134},
  month = oct,
  year = {2018},
  copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  abstract = {}
}

@inproceedings{Storey2022,
  doi = {10.1145/3528579.3529177},
  title = {How developers and managers define and trade productivity for quality},
  booktitle = chase,
  author = {Storey, Margaret-Anne and Houck, Brian and Zimmermann, Thomas},
  publisher = {ACM},
  month = may,
  year = {2022},
  conference = icse,
  abstract = {Background: Developer productivity and software quality are different but related multi-dimensional lenses into the software engineering process. The terms are used liberally in industry settings, but there is a lack of consensus and awareness of what these terms mean in specific contexts and which trade-offs should be considered. Objective & Method: Through an exploratory survey study with developers and managers at Microsoft, we investigated how these cohorts define productivity and quality, how aligned they are in their views, how aware they are of other views, and if and how they trade quality for productivity. Results: We find developers and managers, as cohorts, are not well-aligned in their views of productivity–developers think more about work activities, while more managers consider performance or quality outcomes. We find developers and managers have more aligned views of what quality means, with the majority defining quality in terms of robustness, while the timely delivery of evolvable features that delight users are also key quality aspects. Over half of the developers and managers we surveyed make productivity and quality trade-offs but with good reasons for doing so. Conclusion: Alignment on how developers and managers define productivity and quality is essential if they are to design effective improvement interventions and meaningful metrics to measure productivity and quality improvements. Our research provides a frame for developers and managers to align their views and to make informed decisions on productivity and quality trade-offs.}
}

@article{Strode2022,
  doi = {10.1007/s10664-021-10115-0},
  title = {A teamwork effectiveness model for agile software development},
  author = {Strode, Diane and Dings{\o}yr, Torgeir and Lindsjorn, Yngve},
  abstract = {AbstractTeamwork is crucial in software development, particularly in agile development teams which are cross-functional and where team members work intensively together to develop a cohesive software solution. Effective teamwork is not easy; prior studies indicate challenges with communication, learning, prioritization, and leadership. Nevertheless, there is much advice available for teams, from agile methods, practitioner literature, and general studies on teamwork to a growing body of empirical studies on teamwork in the specific context of agile software development. This article presents the agile teamwork effectiveness model (ATEM) for colocated agile development teams. The model is based on evidence from focus groups, case studies, and multi-vocal literature and is a revision of a general team effectiveness model. Our model of agile teamwork effectiveness is composed of shared leadership, team orientation, redundancy, adaptability, and peer feedback. Coordinating mechanisms are needed to facilitate these components. The coordinating mechanisms are shared mental models, communication, and mutual trust. We critically examine the model and discuss extensions for very small, multi-team, distributed, and safety-critical development contexts. The model is intended for researchers, team members, coaches, and leaders in the agile community.},
  journal = {Empir. Softw. Eng.},
  publisher = {Springer Science and Business Media LLC},
  volume = {27},
  number = {2},
  month = mar,
  year = {2022},
  copyright = {https://creativecommons.org/licenses/by/4.0}
}

@inproceedings{Tian2022,
  doi = {10.1145/3510003.3510205},
  title = {What makes a good commit message?},
  booktitle = icse,
  author = {Tian, Yingchen and Zhang, Yuxia and Stol, Klaas-Jan and Jiang, Lin and Liu, Hui},
  publisher = {ACM},
  month = may,
  year = {2022},
  conference = icse,
  abstract = {A key issue in collaborative software development is communication among developers. One modality of communication is a commit message, in which developers describe the changes they make in a repository. As such, commit messages serve as an “audit trail” by which developers can understand how the source code of a project has changed-and why. Hence, the quality of commit messages affects the effectiveness of communication among developers. Commit messages are often of poor quality as developers lack time and motivation to craft a good message. Several automatic approaches have been proposed to generate commit messages. However, these are based on uncurated datasets including considerable proportions of poorly phrased commit messages. In this multi-method study, we first define what constitutes a “good” commit message, and then establish what proportion of commit messages lack information using a sample of almost 1,600 messages from five highly active open source projects. We find that an average of circa 44% of messages could be improved, suggesting the use of uncurated datasets may be a major threat when commit message generators are trained with such data. We also observe that prior work has not considered semantics of commit messages, and there is surprisingly little guidance available for writing good commit messages. To that end, we develop a taxonomy based on recurring patterns in commit messages' expressions. Finally, we investigate whether “good” commit messages can be automatically identified; such automation could prompt developers to write better commit messages.}
}

@article{Tissenbaum2021,
  doi = {10.1111/bjet.13072},
  title = {The case for alternative endpoints in computing education},
  author = {Tissenbaum, Mike and Weintrop, David and Holbert, Nathan and Clegg, Tamara},
  journal = {Br. J. Educ. Technol.},
  publisher = {Wiley},
  volume = {52},
  number = {3},
  pages = {1164--1177},
  month = may,
  year = {2021},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  abstract = {Funding information NSF, Grant/Award Number: #1441184 Abstract This paper argues for a reexamination of the nature and goals of broad computing education initiatives. Instead of starting with specific values or goals, we instead begin by considering various desired endpoints of computing instruction and then work backward to reason about what form learning activities might take and what are the underlying values and principles that support learners in reaching these endpoints. The result of this exercise is a push for rethinking the form of contemporary computing education with an eye toward more diverse, equitable and meaningful endpoints. With a focus on the role that constructionistfocused pedagogies and designs can play in supporting these endpoints, we examine four distinct cases and the endpoints they support. This paper is not intended to encompass all the possible alternate endpoints for computer science education; rather, this work seeks to start a conversation around the nature of and need for alternate endpoints, as a means to reevaluate the current tools and curricula to prepare learners for a future of active and empowered computingliterate citizens.}
}

@inproceedings{Truong2022,
  doi = {10.1145/3524842.3528488},
  title = {The unsolvable problem or the unheard answer?},
  booktitle = msr,
  author = {Truong, Kimberly and Miller, Courtney and Vasilescu, Bogdan and K{\"a}stner, Christian},
  publisher = {ACM},
  month = may,
  year = {2022},
  abstract = {Talks at practitioner-focused open-source software conferences are a valuable source of information for software engineering researchers. They provide a pulse of the community and are valuable source material for grey literature analysis. We curated a dataset of 24,669 talks from 87 open-source conferences between 2010 and 2021. We stored all relevant metadata from these conferences and provide scripts to collect the transcripts. We believe this data is useful for answering many kinds of questions, such as: What are the important/highly discussed topics within practitioner communities? How do practitioners interact? And how do they present themselves to the public? We demonstrate the usefulness of this data by reporting our findings from two small studies: a topic model analysis providing an overview of open-source community dynamics since 2011 and a qualitative analysis of a smaller community-oriented sample within our dataset to gain a better understanding of why contributors leave open-source software.}
}

@inproceedings{Tshukudu2020,
  doi = {10.1145/3372782.3406270},
  title = {Understanding conceptual transfer for students learning new programming languages},
  booktitle = icer,
  author = {Tshukudu, Ethel and Cutts, Quintin},
  publisher = {ACM},
  month = aug,
  year = {2020},
  conference = icer,
  abstract = {Prior research has shown that students face transition challenges between programming languages (PL) over the course of their education. We could not find research attempting to devise a model that describes the transition process and how students' learning of programming concepts is affected during the shift. In this paper, we propose a model to describe PL transfer for relative novices. In the model, during initial stages of learning a new language, students will engage in learning three categories of concepts, True Carryover Concepts, False Carryover Concepts, or Abstract True Carryover Concepts; during the transition, learners automatically effect a transfer of semantics between languages based on syntax matching. In order to find support for the model, we conducted two empirical studies. Study 1 investigated near-novice undergraduate students transitioning from procedural Python to object-oriented Java while Study 2 investigated near-novice postgraduate students doing a transfer from object-oriented Java to procedural Python. Results for both studies indicate that students had little or no difficulty with transitioning on TCC due to positive semantic transfer based on syntax similarities while they had the most difficulty transitioning on FCC due to negative semantic transfer. Students had little or no semantic transfer on ATCC due to differences in syntax between the languages. We suggest ways in which the model can inform pedagogy on how to ease the transition process.}
}

@inproceedings{Tuna2022,
  doi = {10.1145/3510457.3513080},
  title = {Bug tracking process smells in practice},
  booktitle = icse,
  author = {Tuna, Erdem and Kovalenko, Vladimir and T{\"u}z{\"u}n, Eray},
  publisher = {ACM},
  month = may,
  year = {2022},
  conference = icse,
  abstract = {Software teams use bug tracking (BT) tools to report and manage bugs. Each record in a bug tracking system (BTS) is a reporting entity consisting of several information fields. The contents of the reports are similar across different tracking tools, though not the same. The variation in the workflow between teams prevents defining an ideal process of running BTS. Nevertheless, there are best practices reported both in white and gray literature. Developer teams may not adopt the best practices in their BT process. This study investigates the non-compliance of developers with best practices, so-called smells, in the BT process. We mine bug reports of four projects in the BTS of JetBrains, a software company, to observe the prevalence of BT smells in an industrial setting. Also, we survey developers to see (1) if they recognize the smells, (2) their perception of the severity of the smells, and (3) the potential benefits of a BT process smell detection tool. We found that (1) smells occur, and their detection requires a solid understanding of the BT practices of the projects, (2) smell severity perception varies across smell types, and (3) developers considered that a smell detection tool would be useful for six out of the 12 smell categories.}
}

@article{Wang2020,
  doi = {10.1145/3387111},
  title = {Unveiling elite developers' activities in open source projects},
  author = {Wang, Zhendong and Feng, Yang and Wang, Yi and Jones, James A and Redmiles, David},
  abstract = {Open source developers, particularly the elite developers who own the administrative privileges for a project, maintain a diverse portfolio of contributing activities. They not only commit source code but also exert significant efforts on other communicative, organizational, and supportive activities. However, almost all prior research focuses on specific activities and fails to analyze elite developers' activities in a comprehensive way. To bridge this gap, we conduct an empirical study with fine-grained event data from 20 large open source projects hosted on G IT H UB . We investigate elite developers' contributing activities and their impacts on project outcomes. Our analyses reveal three key findings: (1) elite developers participate in a variety of activities, of which technical contributions (e.g., coding) only account for a small proportion; (2) as the project grows, elite developers tend to put more effort into supportive and communicative activities and less effort into coding; and (3) elite developers' efforts in nontechnical activities are negatively correlated with the project's outcomes in terms of productivity and quality in general, except for a positive correlation with the bug fix rate (a quality indicator). These results provide an integrated view of elite developers' activities and can inform an individual's decision making about effort allocation, which could lead to improved project outcomes. The results also provide implications for supporting these elite developers.},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {29},
  number = {3},
  pages = {1--35},
  month = jul,
  year = {2020},
  abstract = {Open source developers, particularly the elite developers who own the administrative privileges for a project, maintain a diverse portfolio of contributing activities. They not only commit source code but also exert significant efforts on other communicative, organizational, and supportive activities. However, almost all prior research focuses on specific activities and fails to analyze elite developers’ activities in a comprehensive way. To bridge this gap, we conduct an empirical study with fine-grained event data from 20 large open source projects hosted on GITHUB. We investigate elite developers’ contributing activities and their impacts on project outcomes. Our analyses reveal three key findings: (1) elite developers participate in a variety of activities, of which technical contributions (e.g., coding) only account for a small proportion; (2) as the project grows, elite developers tend to put more effort into supportive and communicative activities and less effort into coding; and (3) elite developers’ efforts in nontechnical activities are negatively correlated with the project’s outcomes in terms of productivity and quality in general, except for a positive correlation with the bug fix rate (a quality indicator). These results provide an integrated view of elite developers’ activities and can inform an individual’s decision making about effort allocation, which could lead to improved project outcomes. The results also provide implications for supporting these elite developers.}
}

@inproceedings{Zhang2022,
  doi = {10.1145/3522664.3528620},
  title = {Code smells for machine learning applications},
  booktitle = {Proceedings of the 1st International Conference on {AI} Engineering: Software Engineering for {AI}},
  author = {Zhang, Haiyin and Cruz, Lu{\'\i}s and van Deursen, Arie},
  publisher = {ACM},
  month = may,
  year = {2022},
  conference = {CAIN '22: 1st Conference on AI Engineering - Software Engineering for AI},
  abstract = {The popularity of machine learning has wildly expanded in recent years. Machine learning techniques have been heatedly studied in academia and applied in the industry to create business value. However, there is a lack of guidelines for code quality in machine learning applications. In particular, code smells have rarely been studied in this domain. Although machine learning code is usually integrated as a small part of an overarching system, it usually plays an important role in its core functionality. Hence ensuring code quality is quintessential to avoid issues in the long run. This paper proposes and identifies a list of 22 machine learning-specific code smells collected from various sources, including papers, grey literature, GitHub commits, and Stack Overflow posts. We pinpoint each smell with a description of its context, potential issues in the long run, and proposed solutions. In addition, we link them to their respective pipeline stage and the evidence from both academic and grey literature. The code smell catalog helps data scientists and developers produce and maintain high-quality machine learning application code.}
}

@article{Zheng2019,
  doi = {10.1016/j.jss.2019.02.025},
  title = {Towards understanding bugs in an open source cloud management stack: An empirical study of {OpenStack} software bugs},
  author = {Zheng, Wei and Feng, Chen and Yu, Tingting and Yang, Xibing and Wu, Xiaoxue},
  journal = {J. Syst. Softw.},
  publisher = {Elsevier BV},
  volume = {151},
  pages = {210--223},
  month = may,
  year = {2019},
  abstract = {}
}

@article{Zhou2020,
  doi = {10.1007/s10664-019-09744-3},
  title = {Bounties on technical {Q\&A} sites: a case study of Stack Overflow bounties},
  author = {Zhou, Jiayuan and Wang, Shaowei and Bezemer, Cor-Paul and Hassan, Ahmed E},
  journal = {Empir. Softw. Eng.},
  publisher = {Springer Science and Business Media LLC},
  volume = {25},
  number = {1},
  pages = {139--177},
  month = jan,
  year = {2020},
  abstract = {}
}
